"""
å˜å‹å™¨å†·å´ç¯å¢ƒ - å®Œå…¨åŸºäºé™æ¸©èƒ½åŠ›è¯„ä»·ï¼ˆç§»é™¤target_tempï¼‰

æ ¸å¿ƒæ”¹è¿›ï¼š
1. âœ… å®Œå…¨ç§»é™¤target_tempå±æ€§
2. âœ… ä½¿ç”¨CONFIGä¸­çš„é™æ¸©èƒ½åŠ›å‚æ•°
3. âœ… åŠ¨æ€é™æ¸©ç›®æ ‡ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple
from config import CONFIG


class ImprovedTransformerCoolingEnv:
    """
    æ”¹è¿›çš„å˜å‹å™¨å†·å´æ§åˆ¶ç¯å¢ƒ - çº¯é™æ¸©èƒ½åŠ›è¯„ä»·
    """

    def __init__(self, data: pd.DataFrame, start_idx: int = 0):
        """
        åˆå§‹åŒ–ç¯å¢ƒ

        Args:
            data: å¤„ç†åçš„æ•°æ®DataFrame
            start_idx: èµ·å§‹ç´¢å¼•
        """
        self.data = data
        self.start_idx = start_idx
        self.current_idx = start_idx

        # ğŸ”¥ ä»CONFIGè¯»å–å‚æ•°
        self.max_steps = CONFIG.env.MAX_STEPS
        self.state_dim = CONFIG.env.STATE_DIM
        self.action_dim = CONFIG.env.ACTION_DIM

        # ç¯å¢ƒç‰©ç†å‚æ•°ï¼ˆä»CONFIGè¯»å–ï¼‰
        self.water_temp = CONFIG.env.WATER_TEMP
        self.tank_capacity = CONFIG.env.TANK_CAPACITY
        self.nozzle_count = CONFIG.env.NOZZLE_COUNT
        self.peltier_power = CONFIG.env.PELTIER_POWER

        # ğŸ”¥ æ¸©åº¦åŒºé—´é˜ˆå€¼ï¼ˆä»CONFIGè¯»å–ï¼Œç”¨äºç¡®å®šé™æ¸©ç›®æ ‡ï¼‰
        self.temp_low = CONFIG.env.TEMP_LOW
        self.temp_medium = CONFIG.env.TEMP_MEDIUM
        self.temp_high = CONFIG.env.TEMP_HIGH

        # çŠ¶æ€å’ŒåŠ¨ä½œ
        self.current_state = None
        self.last_action = None
        self.last_oil_temp = None  # ğŸ”¥ è®°å½•ä¸Šä¸€æ­¥æ²¹æ¸©ï¼Œç”¨äºè®¡ç®—é™æ¸©å¹…åº¦
        self.step_count = 0

        # è®°å½•
        self.episode_data = {
            'states': [],
            'actions': [],
            'rewards': [],
            'oil_temps': [],
            'cooling_amounts': [],  # ğŸ”¥ è®°å½•æ¯æ­¥å®é™…é™æ¸©é‡
            'target_coolings': [],  # ğŸ”¥ è®°å½•æ¯æ­¥ç›®æ ‡é™æ¸©é‡
            'ambient_temps': [],
            'reward_components': []
        }

    def get_cooling_target(self, oil_temp: float) -> float:
        """
        ğŸ”¥ æ ¹æ®æ²¹æ¸©ç¡®å®šé™æ¸©ç›®æ ‡ï¼ˆä½¿ç”¨CONFIGï¼‰

        Args:
            oil_temp: å½“å‰æ²¹æ¸©

        Returns:
            ç›®æ ‡é™æ¸©é‡ï¼ˆÂ°Cï¼‰
        """
        return CONFIG.env.get_cooling_target(oil_temp)

    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_idx = self.start_idx
        self.step_count = 0
        self.last_action = None
        self.last_oil_temp = None  # ğŸ”¥ é‡ç½®ä¸Šä¸€æ­¥æ²¹æ¸©

        # æ¸…ç©ºè®°å½•
        self.episode_data = {
            'states': [],
            'actions': [],
            'rewards': [],
            'oil_temps': [],
            'cooling_amounts': [],
            'target_coolings': [],
            'ambient_temps': [],
            'reward_components': []
        }

        # è·å–åˆå§‹çŠ¶æ€
        self.current_state = self._get_state()
        self.last_oil_temp = self.data.iloc[self.current_idx]['oil_temp']

        return self.current_state

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ

        Args:
            action: åŠ¨ä½œæ•°ç»„ [pump_pressure, peltier_on, valve_opening]

        Returns:
            next_state, reward, done, info
        """
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = self._clip_action(action)

        # è®¡ç®—å†·å´æ•ˆæœ
        cooling_effect = self._calculate_cooling_effect(action)

        # è·å–å½“å‰æ²¹æ¸©å’Œç¯å¢ƒæ¸©åº¦
        current_oil_temp = self.data.iloc[self.current_idx]['oil_temp']
        ambient_temp = self.data.iloc[self.current_idx]['ambient_temp']

        # æ›´æ–°ç´¢å¼•
        self.current_idx += 1
        if self.current_idx >= len(self.data):
            self.current_idx = len(self.data) - 1

        next_oil_temp = self.data.iloc[self.current_idx]['oil_temp']

        # åº”ç”¨å†·å´æ•ˆæœ
        actual_oil_temp = next_oil_temp - cooling_effect + np.random.normal(0, 0.5)

        # æ›´æ–°æ•°æ®
        self.data.at[self.data.index[self.current_idx], 'oil_temp'] = actual_oil_temp

        # ğŸ”¥ è®¡ç®—å®é™…é™æ¸©é‡
        if self.last_oil_temp is not None:
            actual_cooling = self.last_oil_temp - actual_oil_temp
        else:
            actual_cooling = 0

        # ğŸ”¥ è·å–ç›®æ ‡é™æ¸©é‡ï¼ˆä½¿ç”¨CONFIGï¼‰
        target_cooling = self.get_cooling_target(
            self.last_oil_temp if self.last_oil_temp is not None else current_oil_temp
        )

        # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€
        next_state = self._get_state()

        # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒï¼šè®¡ç®—åŸºäºé™æ¸©èƒ½åŠ›çš„å¥–åŠ±ï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰
        reward, reward_info = self._calculate_cooling_based_reward(
            action, actual_cooling, target_cooling, actual_oil_temp, ambient_temp
        )

        # è®°å½•æ•°æ®
        self.episode_data['states'].append(self.current_state)
        self.episode_data['actions'].append(action)
        self.episode_data['rewards'].append(reward)
        self.episode_data['oil_temps'].append(actual_oil_temp)
        self.episode_data['cooling_amounts'].append(actual_cooling)  # ğŸ”¥ è®°å½•å®é™…é™æ¸©
        self.episode_data['target_coolings'].append(target_cooling)  # ğŸ”¥ è®°å½•ç›®æ ‡é™æ¸©
        self.episode_data['ambient_temps'].append(ambient_temp)
        self.episode_data['reward_components'].append(reward_info)

        # æ›´æ–°çŠ¶æ€
        self.current_state = next_state
        self.last_action = action.copy()
        self.last_oil_temp = actual_oil_temp  # ğŸ”¥ æ›´æ–°ä¸Šä¸€æ­¥æ²¹æ¸©
        self.step_count += 1

        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.step_count >= self.max_steps

        # é™„åŠ ä¿¡æ¯
        info = {
            'oil_temp': actual_oil_temp,
            'ambient_temp': ambient_temp,
            'cooling_effect': cooling_effect,
            'actual_cooling': actual_cooling,  # ğŸ”¥ å®é™…é™æ¸©é‡
            'target_cooling': target_cooling,  # ğŸ”¥ ç›®æ ‡é™æ¸©é‡
            'cooling_error': abs(actual_cooling - target_cooling),  # ğŸ”¥ é™æ¸©è¯¯å·®
            'step': self.step_count,
            **reward_info
        }

        return next_state, reward, done, info

    def _calculate_cooling_based_reward(
            self,
            action: np.ndarray,
            actual_cooling: float,
            target_cooling: float,
            oil_temp: float,
            ambient_temp: float
    ) -> Tuple[float, Dict]:
        """
        ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šè®¡ç®—åŸºäºé™æ¸©èƒ½åŠ›çš„å¥–åŠ±å‡½æ•°ï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰

        å¥–åŠ±æƒé‡æ¥è‡ªCONFIG.reward:
        - é™æ¸©æ•ˆæœ: 90%
        - èƒ½è€—æƒ©ç½š: 8%
        - å¹³æ»‘æ€§: 2%

        Args:
            action: æ§åˆ¶åŠ¨ä½œ
            actual_cooling: å®é™…é™æ¸©é‡ï¼ˆÂ°Cï¼‰
            target_cooling: ç›®æ ‡é™æ¸©é‡ï¼ˆÂ°Cï¼‰
            oil_temp: å½“å‰æ²¹æ¸©
            ambient_temp: ç¯å¢ƒæ¸©åº¦

        Returns:
            (total_reward, reward_info)
        """
        # 1. ğŸ”¥ é™æ¸©æ•ˆæœå¥–åŠ±ï¼ˆä¸»è¦ï¼‰- ä½¿ç”¨CONFIGæƒé‡
        cooling_error = abs(actual_cooling - target_cooling)

        if cooling_error < CONFIG.reward.EXCELLENT_COOLING_ERROR:  # ä½¿ç”¨CONFIGé˜ˆå€¼
            # éå¸¸ç²¾ç¡®çš„é™æ¸©æ§åˆ¶
            cooling_reward = 100 * np.exp(-0.5 * cooling_error)  # [90, 100]
        elif cooling_error < CONFIG.reward.GOOD_COOLING_ERROR:  # ä½¿ç”¨CONFIGé˜ˆå€¼
            # è‰¯å¥½çš„é™æ¸©æ§åˆ¶
            cooling_reward = 60 * np.exp(-0.3 * cooling_error)  # [20, 60]
        else:
            # é™æ¸©åå·®è¾ƒå¤§ï¼Œæœ‰ç•Œæƒ©ç½š
            cooling_reward = -20 * np.tanh(cooling_error / 5.0)  # [-20, 0]

        # 2. èƒ½è€—æƒ©ç½šï¼ˆæ¬¡è¦ï¼‰
        energy_penalty = self._calculate_energy_penalty_light(action)

        # 3. å¹³æ»‘æ€§å¥–åŠ±ï¼ˆè¾…åŠ©ï¼‰
        smoothness_reward = self._calculate_smoothness_reward_light(action)

        # 4. å®‰å…¨å¥–åŠ±ï¼ˆä½¿ç”¨CONFIGé˜ˆå€¼ï¼‰
        if oil_temp > CONFIG.reward.SAFETY_TEMP_THRESHOLD:
            safety_penalty = -10 * (oil_temp - CONFIG.reward.SAFETY_TEMP_THRESHOLD)
        else:
            safety_penalty = 0

        # 5. åŠ æƒæ€»å¥–åŠ±ï¼ˆä½¿ç”¨CONFIGæƒé‡ï¼‰
        total_reward = (
                CONFIG.reward.COOLING_REWARD_WEIGHT * cooling_reward +
                CONFIG.reward.ENERGY_PENALTY_WEIGHT * energy_penalty +
                CONFIG.reward.SMOOTHNESS_REWARD_WEIGHT * smoothness_reward +
                safety_penalty
        )

        # è¯¦ç»†ä¿¡æ¯
        reward_info = {
            'cooling_reward': cooling_reward,
            'energy_penalty': energy_penalty,
            'smoothness_reward': smoothness_reward,
            'safety_penalty': safety_penalty,
            'total_reward': total_reward,
            'cooling_error': cooling_error,
            'actual_cooling': actual_cooling,
            'target_cooling': target_cooling
        }

        return total_reward, reward_info

    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        if self.current_idx >= len(self.data):
            self.current_idx = len(self.data) - 1

        row = self.data.iloc[self.current_idx]

        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        state = row[numeric_cols].values.astype(np.float32)

        # ç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
        if len(state) < self.state_dim:
            state = np.pad(state, (0, self.state_dim - len(state)), 'constant')
        elif len(state) > self.state_dim:
            state = state[:self.state_dim]

        return state

    def _clip_action(self, action: np.ndarray) -> np.ndarray:
        """è£å‰ªåŠ¨ä½œåˆ°æœ‰æ•ˆèŒƒå›´ï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰"""
        clipped_action = np.array([
            np.clip(action[0], CONFIG.env.PUMP_PRESSURE_MIN, CONFIG.env.PUMP_PRESSURE_MAX),
            np.clip(action[1], CONFIG.env.PELTIER_MIN, CONFIG.env.PELTIER_MAX),
            np.clip(action[2], CONFIG.env.VALVE_OPENING_MIN, CONFIG.env.VALVE_OPENING_MAX)
        ])
        return clipped_action

    def _calculate_cooling_effect(self, action: np.ndarray) -> float:
        """è®¡ç®—å†·å´æ•ˆæœï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰"""
        pump_pressure = action[0]
        peltier_on = action[1]
        valve_opening = action[2]

        # æ°´å†·æ•ˆæœ
        water_cooling = (pump_pressure - CONFIG.env.PUMP_PRESSURE_MIN) / \
                        (CONFIG.env.PUMP_PRESSURE_MAX - CONFIG.env.PUMP_PRESSURE_MIN) * \
                        valve_opening / 100.0 * 3.0

        # å¸•å°”è´´å†·å´æ•ˆæœ
        peltier_cooling = peltier_on * self.peltier_power / 100.0 * 1.5

        # æ€»å†·å´æ•ˆæœ
        total_cooling = water_cooling + peltier_cooling

        return total_cooling

    def _calculate_energy_penalty_light(self, action: np.ndarray) -> float:
        """è®¡ç®—è½»é‡çº§èƒ½è€—æƒ©ç½šï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰"""
        pump_pressure = action[0]
        peltier_on = action[1]
        valve_opening = action[2]

        # è®¡ç®—å„æ‰§è¡Œå™¨åŠŸç‡
        pump_power = (pump_pressure - CONFIG.env.PUMP_PRESSURE_MIN) / \
                     (CONFIG.env.PUMP_PRESSURE_MAX - CONFIG.env.PUMP_PRESSURE_MIN) * 100
        peltier_power = peltier_on * self.peltier_power
        valve_power = valve_opening / 100 * 50

        # è½»é‡çº§åŠ æƒèƒ½è€—
        total_energy = (0.3 * pump_power +
                        0.5 * peltier_power +
                        0.1 * valve_power)

        # è¿”å›è´Ÿçš„èƒ½è€—ï¼ˆä½œä¸ºæƒ©ç½šï¼‰
        return -total_energy * 0.05

    def _calculate_smoothness_reward_light(self, action: np.ndarray) -> float:
        """è®¡ç®—è½»é‡çº§æ§åˆ¶å¹³æ»‘æ€§å¥–åŠ±"""
        if self.last_action is None:
            return 0.0

        # è®¡ç®—åŠ¨ä½œå˜åŒ–çš„æ¬§æ°è·ç¦»
        action_change = np.linalg.norm(action - self.last_action)

        # è¿”å›è´Ÿçš„å¹³æ»‘æ€§æƒ©ç½š
        smoothness_penalty = -action_change * 0.5

        return smoothness_penalty

    def get_episode_data(self) -> Dict:
        """è·å–episodeæ•°æ®"""
        return self.episode_data

    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ"""
        if self.current_state is not None:
            oil_temp = self.data.iloc[self.current_idx]['oil_temp']
            target_cooling = self.get_cooling_target(oil_temp)
            print(f"Step: {self.step_count}, Oil Temp: {oil_temp:.2f}Â°C, "
                  f"Target Cooling: {target_cooling:.2f}Â°C (æ¥è‡ªCONFIG)")


# ä¿æŒå‘åå…¼å®¹
class MultiEpisodeEnv:
    """å¤šEpisodeç¯å¢ƒç®¡ç†å™¨"""

    def __init__(self, data: pd.DataFrame, train_dates: list, use_improved=True):
        """
        åˆå§‹åŒ–å¤šEpisodeç¯å¢ƒ

        Args:
            data: å®Œæ•´æ•°æ®DataFrame
            train_dates: è®­ç»ƒæ—¥æœŸåˆ—è¡¨
            use_improved: æ˜¯å¦ä½¿ç”¨æ”¹è¿›çš„ç¯å¢ƒï¼ˆé»˜è®¤Trueï¼‰
        """
        self.data = data
        self.train_dates = train_dates
        self.envs = {}
        self.use_improved = use_improved

        # ä¸ºæ¯ä¸ªæ—¥æœŸåˆ›å»ºç¯å¢ƒ
        for date in train_dates:
            start_idx = self._find_date_index(date)
            if start_idx >= 0:
                self.envs[date] = ImprovedTransformerCoolingEnv(data.copy(), start_idx)

        self.current_date = None
        self.current_env = None

    def _find_date_index(self, date_str: str) -> int:
        """æŸ¥æ‰¾æ—¥æœŸå¯¹åº”çš„ç´¢å¼•"""
        try:
            target_date = pd.to_datetime(date_str)
            time_diff = abs(self.data.index - target_date)
            idx = time_diff.argmin()
            return idx
        except:
            return -1

    def reset(self, date: str = None) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        if date is None:
            date = np.random.choice(self.train_dates)

        self.current_date = date
        self.current_env = self.envs[date]

        return self.current_env.reset()

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        return self.current_env.step(action)

    def get_episode_data(self) -> Dict:
        """è·å–å½“å‰episodeæ•°æ®"""
        return self.current_env.get_episode_data()

    def get_all_envs(self) -> Dict:
        """è·å–æ‰€æœ‰ç¯å¢ƒ"""
        return self.envs


if __name__ == "__main__":
    # æµ‹è¯•æ–°çš„é™æ¸©èƒ½åŠ›è¯„ä»·ç¯å¢ƒ
    print("=" * 80)
    print("æµ‹è¯•ç¯å¢ƒæ¨¡å—ï¼ˆå®Œå…¨ä½¿ç”¨CONFIGï¼Œæ— target_tempï¼‰".center(80))
    print("=" * 80)

    print("\nâœ… æ ¸å¿ƒæ”¹è¿›ï¼š")
    print("  1. âœ… å®Œå…¨ç§»é™¤target_tempå±æ€§")
    print("  2. âœ… æ‰€æœ‰å‚æ•°ä»CONFIGè¯»å–")
    print("  3. âœ… ä½¿ç”¨CONFIG.env.get_cooling_target()è·å–é™æ¸©ç›®æ ‡")
    print("  4. âœ… å¥–åŠ±æƒé‡ä»CONFIG.rewardè¯»å–")
    print("  5. âœ… æ¸©åº¦é˜ˆå€¼ä»CONFIG.envè¯»å–")

    print("\nğŸ“Š CONFIGå‚æ•°å±•ç¤ºï¼š")
    print(f"  MAX_STEPS = {CONFIG.env.MAX_STEPS}")
    print(f"  TEMP_LOW = {CONFIG.env.TEMP_LOW}Â°C")
    print(f"  TEMP_MEDIUM = {CONFIG.env.TEMP_MEDIUM}Â°C")
    print(f"  TEMP_HIGH = {CONFIG.env.TEMP_HIGH}Â°C")
    print(f"  COOLING_REWARD_WEIGHT = {CONFIG.reward.COOLING_REWARD_WEIGHT}")
    print(f"  ENERGY_PENALTY_WEIGHT = {CONFIG.reward.ENERGY_PENALTY_WEIGHT}")
    print(f"  SMOOTHNESS_REWARD_WEIGHT = {CONFIG.reward.SMOOTHNESS_REWARD_WEIGHT}")

    print("\nåˆ›å»ºæµ‹è¯•æ•°æ®...")
    n_hours = CONFIG.env.MAX_STEPS
    time_index = pd.date_range(start='2024-07-01', periods=n_hours, freq='H')
    data = pd.DataFrame(index=time_index)

    # æ¨¡æ‹Ÿæ²¹æ¸©ä»50Â°Cé€æ¸å‡é«˜åˆ°80Â°Cï¼ˆè·¨è¶Šæ‰€æœ‰æ¸©åº¦åŒºé—´ï¼‰
    data['oil_temp'] = np.linspace(50, 80, n_hours) + np.random.normal(0, 2, n_hours)
    data['ambient_temp'] = 30 + 5 * np.sin(2 * np.pi * np.arange(n_hours) / 24)
    data['humidity'] = 60 + np.random.normal(0, 5, n_hours)
    for i in range(CONFIG.env.STATE_DIM - 3):
        data[f'feature_{i}'] = np.random.randn(n_hours)

    print("âœ“ æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")

    print("\nåˆ›å»ºç¯å¢ƒ...")
    env = ImprovedTransformerCoolingEnv(data, start_idx=0)
    print("âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"âœ“ ä¸å†æœ‰target_tempå±æ€§")
    print(f"âœ“ ä½¿ç”¨get_cooling_target()æ–¹æ³•åŠ¨æ€è·å–é™æ¸©ç›®æ ‡")

    print("\næ‰§è¡Œæµ‹è¯•...")
    state = env.reset()

    for i in range(5):
        # éšæœºåŠ¨ä½œ
        action = np.random.uniform([2.0, 0.0, 0.0], [5.0, 1.0, 100.0])
        next_state, reward, done, info = env.step(action)

        print(f"\nStep {i + 1}:")
        print(f"  æ²¹æ¸©:         {info['oil_temp']:5.2f}Â°C")
        print(f"  å®é™…é™æ¸©:     {info['actual_cooling']:+5.2f}Â°C")
        print(f"  ç›®æ ‡é™æ¸©:     {info['target_cooling']:5.2f}Â°C (æ¥è‡ªCONFIG)")
        print(f"  é™æ¸©è¯¯å·®:     {info['cooling_error']:5.2f}Â°C")
        print(f"  å¥–åŠ±:         {reward:7.2f}")

    print("\n" + "=" * 80)
    print("âœ“ ç¯å¢ƒæ¨¡å—æµ‹è¯•å®Œæˆï¼ˆå®Œå…¨ä½¿ç”¨CONFIGï¼Œæ— target_tempï¼‰".center(80))
    print("=" * 80)

    print("\nğŸ“‹ é™æ¸©ç›®æ ‡è§„åˆ™æµ‹è¯•ï¼ˆæ¥è‡ªCONFIGï¼‰:")
    test_temps = [50, 60, 70, 80]
    for temp in test_temps:
        target = env.get_cooling_target(temp)
        print(f"  æ²¹æ¸© {temp}Â°C â†’ ç›®æ ‡é™æ¸© {target}Â°C")