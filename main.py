"""
10kVå˜å‹å™¨æ™ºèƒ½å†·å´æ§åˆ¶ç³»ç»Ÿ - ä¸»ç¨‹åºï¼ˆæ”¹è¿›ç‰ˆï¼‰
Main Program with Clear Multi-Algorithm Training

æ”¹è¿›è¦ç‚¹ï¼š
1. âœ… æ¸…æ™°æ˜¾ç¤ºå°†è®­ç»ƒå“ªäº›ç®—æ³•
2. âœ… æ˜ç¡®æ˜¾ç¤ºä½¿ç”¨çš„æ•°æ®æºï¼ˆçœŸå®/æ¨¡æ‹Ÿï¼‰
3. âœ… æ·»åŠ æ•°æ®æ£€æŸ¥å’Œç¡®è®¤æç¤º
4. âœ… ä¼˜åŒ–è®­ç»ƒæµç¨‹å’Œè¾“å‡ºä¿¡æ¯
"""

import os
import sys
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import warnings
from scipy.io import savemat

warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import CONFIG
from environment import ImprovedTransformerCoolingEnv, MultiEpisodeEnv
from sac_temperature_aware import ImprovedSAC
from sac_base import BaseSAC
from ppo import PPO
from ddpg import DDPG
from td3 import TD3
from trainer import Trainer, MultiAlgorithmTrainer
from evaluator import MultiAlgorithmEvaluator, generate_evaluation_csv_files, generate_metrics_table
from metrics import MetricsCalculator


def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("\n" + "=" * 80)
    print("10kVå˜å‹å™¨æ™ºèƒ½å†·å´æ§åˆ¶ç³»ç»Ÿ - å¤šç®—æ³•å¼ºåŒ–å­¦ä¹ è®­ç»ƒ".center(80))
    print("Transformer Cooling System - Multi-Algorithm RL Training".center(80))
    print("=" * 80)


def check_data_files(data_dir: str = 'data') -> dict:
    """
    æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨

    Returns:
        dict: åŒ…å«æ–‡ä»¶æ£€æŸ¥ç»“æœ
    """
    print("\n" + "=" * 80)
    print("æ•°æ®æ–‡ä»¶æ£€æŸ¥".center(80))
    print("=" * 80)

    required_files = {
        'oil_temp': 'Oil_temperature_data_for_July_2024.xlsx',
        'weather': 'Weather_data_for_24_hours_on_July_2024.xlsx',
        'predicted': 'Predicted_temperature_data_for_July_2024.xlsx'
    }

    file_status = {}
    all_exist = True

    for key, filename in required_files.items():
        filepath = os.path.join(data_dir, filename)
        exists = os.path.exists(filepath)
        file_status[key] = exists

        status_icon = "âœ“" if exists else "âœ—"
        status_text = "å­˜åœ¨" if exists else "ç¼ºå¤±"
        color = "\033[92m" if exists else "\033[91m"
        reset = "\033[0m"

        print(f"{color}{status_icon}{reset} {filename:<50} [{status_text}]")

        if not exists:
            all_exist = False

    print("=" * 80)

    if all_exist:
        print("\nâœ“ æ‰€æœ‰çœŸå®æ•°æ®æ–‡ä»¶éƒ½å­˜åœ¨ï¼Œå°†ä½¿ç”¨çœŸå®Excelæ•°æ®è®­ç»ƒ")
        return {'status': 'real', 'files': file_status}
    else:
        print("\nâš  éƒ¨åˆ†æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒ")
        print("æç¤º: å¦‚éœ€ä½¿ç”¨çœŸå®æ•°æ®ï¼Œè¯·å°†Excelæ–‡ä»¶æ”¾ç½®åœ¨ 'data/' ç›®å½•ä¸‹")
        return {'status': 'simulated', 'files': file_status}


def load_and_process_data(data_dir: str = None, force_reload: bool = False):
    """
    åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆçœŸå®æ•°æ®ç‰ˆï¼‰

    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰

    Returns:
        å¤„ç†åçš„æ•°æ®DataFrame
    """
    if data_dir is None:
        data_dir = CONFIG.data.DATA_DIR

    processed_file = os.path.join(data_dir, CONFIG.data.PROCESSED_DATA_FILE)

    # å¦‚æœä¸å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œå°è¯•ä½¿ç”¨ç¼“å­˜
    if not force_reload and os.path.exists(processed_file):
        print(f"\nå‘ç°å·²å¤„ç†çš„æ•°æ®: {processed_file}")
        print("æ­£åœ¨åŠ è½½ç¼“å­˜æ•°æ®...")
        try:
            with open(processed_file, 'rb') as f:
                data_dict = pickle.load(f)
            print("âœ“ æˆåŠŸåŠ è½½ç¼“å­˜æ•°æ®")
            return data_dict['processed_data']
        except Exception as e:
            print(f"âš  ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            print("å°†é‡æ–°å¤„ç†æ•°æ®...")

    # å°è¯•åŠ è½½çœŸå®æ•°æ®
    print("\n" + "=" * 70)
    print("å°è¯•åŠ è½½çœŸå®Excelæ•°æ®".center(70))
    print("=" * 70)

    try:
        # å¯¼å…¥æ•°æ®åŠ è½½å™¨ï¼ˆå‡è®¾å·²åˆ›å»ºï¼‰
        from complete_data_loader import TransformerDataLoader

        loader = TransformerDataLoader(data_dir=data_dir)
        data = loader.load_all_and_process()

        if data is not None and len(data) > 0:
            print("\nâœ“ æˆåŠŸåŠ è½½çœŸå®æ•°æ®ï¼")
            return data
        else:
            raise Exception("æ•°æ®åŠ è½½è¿”å›ç©ºå€¼")

    except ImportError:
        print("\nâš  æœªæ‰¾åˆ°æ•°æ®åŠ è½½å™¨æ¨¡å— (complete_data_loader.py)")
        print("è¯·å…ˆè¿è¡Œæ•°æ®åŠ è½½è„šæœ¬")
    except FileNotFoundError as e:
        print(f"\nâš  Excelæ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print(f"  - {data_dir}/Oil_temperature_data_for_July_2024.xlsx")
        print(f"  - {data_dir}/Weather_data_for_24_hours_on_July_2024.xlsx")
        print(f"  - {data_dir}/Predicted_temperature_data_for_July_2024.xlsx")
    except Exception as e:
        print(f"\nâš  çœŸå®æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # å¦‚æœçœŸå®æ•°æ®åŠ è½½å¤±è´¥ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
    print("\n" + "=" * 70)
    print("âš  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ä¾›æµ‹è¯•ï¼‰".center(70))
    print("=" * 70)
    print("è­¦å‘Š: æ¨¡æ‹Ÿæ•°æ®æ— æ³•ä»£è¡¨çœŸå®åœºæ™¯ï¼")
    print("å»ºè®®: è¯·åŠ è½½çœŸå®Excelæ•°æ®ä»¥è·å¾—å‡†ç¡®ç»“æœ")
    print("=" * 70)

    return generate_simulation_data(data_dir)


def generate_simulation_data(data_dir: str):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    print("\nç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")

    # ç”Ÿæˆ48å°æ—¶çš„æ¨¡æ‹Ÿæ•°æ®
    n_hours = 48 * 30  # 30å¤©æ•°æ®

    # æ—¶é—´åºåˆ—
    time_index = pd.date_range(
        start='2024-07-01 00:00:00',
        periods=n_hours,
        freq='H'
    )

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    data = pd.DataFrame(index=time_index)

    # æ²¹æ¸©ï¼ˆæ­£å¼¦å˜åŒ– + å™ªå£° + è¶‹åŠ¿ï¼‰
    base_temp = 60
    daily_variation = 10 * np.sin(2 * np.pi * np.arange(n_hours) / 24)
    weekly_trend = 5 * np.sin(2 * np.pi * np.arange(n_hours) / (24 * 7))
    noise = np.random.normal(0, 2, n_hours)
    data['oil_temp'] = base_temp + daily_variation + weekly_trend + noise

    # ç¯å¢ƒæ¸©åº¦
    ambient_base = 28
    ambient_variation = 8 * np.sin(2 * np.pi * np.arange(n_hours) / 24 - np.pi / 4)
    data['ambient_temp'] = ambient_base + ambient_variation + np.random.normal(0, 1, n_hours)

    # é¢„æµ‹æ¸©åº¦ï¼ˆæ²¹æ¸© + å°åç§»ï¼‰
    data['predicted_temp'] = data['oil_temp'] + np.random.normal(0, 1, n_hours)

    # æ¹¿åº¦
    data['humidity'] = 60 + np.random.normal(0, 5, n_hours)

    # è´Ÿè½½ç‡
    data['load_rate'] = 0.7 + 0.2 * np.sin(2 * np.pi * np.arange(n_hours) / 24) + \
                        np.random.normal(0, 0.05, n_hours)

    # æ—¶é—´ç‰¹å¾
    data['hour'] = data.index.hour
    data['day'] = data.index.day
    data['month'] = data.index.month

    # æ·»åŠ æ›´å¤šç‰¹å¾ä»¥æ»¡è¶³state_dimè¦æ±‚
    data['oil_temp_error'] = data['oil_temp'] - 50.0
    data['temp_change_rate'] = data['oil_temp'].diff().fillna(0)
    data['oil_temp_ma3'] = data['oil_temp'].rolling(window=3, min_periods=1).mean()
    data['feedforward_signal'] = -(data['predicted_temp'] - 50.0) / 10.0

    for i in range(CONFIG.env.STATE_DIM - len(data.columns)):
        data[f'feature_{i}'] = np.random.randn(n_hours)

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    processed_file = os.path.join(data_dir, CONFIG.data.PROCESSED_DATA_FILE)
    with open(processed_file, 'wb') as f:
        pickle.dump({'processed_data': data}, f)

    print(f"âœ“ æ¨¡æ‹Ÿæ•°æ®å·²ç”Ÿæˆå¹¶ä¿å­˜: {processed_file}")
    print(f"  æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"  æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")

    return data


def confirm_training_config(algorithms: list, num_episodes: int, data_status: str):
    """
    ç¡®è®¤è®­ç»ƒé…ç½®

    Args:
        algorithms: è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨
        num_episodes: è®­ç»ƒepisodes
        data_status: æ•°æ®çŠ¶æ€ï¼ˆ'real' æˆ– 'simulated'ï¼‰

    Returns:
        bool: æ˜¯å¦ç»§ç»­è®­ç»ƒ
    """
    print("\n" + "=" * 80)
    print("è®­ç»ƒé…ç½®ç¡®è®¤".center(80))
    print("=" * 80)

    print(f"\nğŸ“Š æ•°æ®æº: {'âœ“ çœŸå®Excelæ•°æ®' if data_status == 'real' else 'âš  æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®'}")
    print(f"ğŸ¯ è®­ç»ƒEpisodes: {num_episodes}")
    print(f"ğŸ¤– è®­ç»ƒç®—æ³•æ•°é‡: {len(algorithms)}")
    print(f"\nå°†è®­ç»ƒä»¥ä¸‹ç®—æ³•:")

    algorithm_names = {
        'improved_sac': 'Improved SAC (TD3-SACæ··åˆ)',
        'sac': 'SAC (è½¯Actor-Critic)',
        'ppo': 'PPO (è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)',
        'ddpg': 'DDPG (æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦)',
        'td3': 'TD3 (åŒå»¶è¿ŸDDPG)'
    }

    for i, algo in enumerate(algorithms, 1):
        name = algorithm_names.get(algo, algo.upper())
        print(f"  {i}. {name}")

    print("\n" + "=" * 80)

    if data_status == 'simulated':
        print("\nâš  è­¦å‘Š: å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œè®­ç»ƒç»“æœä»…ä¾›å‚è€ƒï¼")
        print("å»ºè®®: ä½¿ç”¨çœŸå®Excelæ•°æ®ä»¥è·å¾—å‡†ç¡®çš„æ¨¡å‹æ€§èƒ½")

    # è‡ªåŠ¨ç»§ç»­ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='10kVå˜å‹å™¨æ™ºèƒ½å†·å´ç³»ç»Ÿ - å¤šç®—æ³•è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è®­ç»ƒæ‰€æœ‰ç®—æ³•ï¼ˆé»˜è®¤ï¼‰
  python main.py --mode full --episodes 100

  # åªè®­ç»ƒç‰¹å®šç®—æ³•
  python main.py --algorithms improved_sac sac --episodes 100

  # åªè¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹
  python main.py --mode eval

  # å¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®
  python main.py --force-reload
        """
    )

    # â­ é»˜è®¤è®­ç»ƒæ‰€æœ‰ç®—æ³•
    parser.add_argument('--algorithms', type=str, nargs='+',
                        default=['improved_sac', 'sac', 'ppo', 'ddpg', 'td3'],
                        choices=['improved_sac', 'sac', 'ppo', 'ddpg', 'td3'],
                        help='è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨ï¼ˆé»˜è®¤ï¼šæ‰€æœ‰ç®—æ³•ï¼‰')

    parser.add_argument('--episodes', type=int, default=None,
                        help='è®­ç»ƒepisodes (None=è‡ªåŠ¨æ ¹æ®æ•°æ®é‡è°ƒæ•´)')

    parser.add_argument('--eval-episodes', type=int, default=10,
                        help='è¯„ä¼°episodes')

    parser.add_argument('--mode', type=str, default='full',
                        choices=['train', 'eval', 'full'],
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), eval(ä»…è¯„ä¼°), full(è®­ç»ƒ+è¯„ä¼°)')

    parser.add_argument('--force-reload', action='store_true',
                        help='å¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰')

    parser.add_argument('--skip-confirmation', action='store_true',
                        help='è·³è¿‡é…ç½®ç¡®è®¤æç¤º')

    args = parser.parse_args()

    # æ‰“å°æ¨ªå¹…
    print_banner()

    # æ‰“å°é…ç½®
    CONFIG.print_config()

    # â­ æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_check = check_data_files()
    data_status = data_check['status']

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    data = load_and_process_data(force_reload=args.force_reload)
    print(f"âœ“ æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"âœ“ æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")

    # â­ è®¡ç®—å¯ç”¨episodes
    available_episodes = len(data) // CONFIG.env.MAX_STEPS
    print(f"âœ“ å¯ç”¨è®­ç»ƒepisodes: {available_episodes}")

    # â­ è‡ªåŠ¨è°ƒæ•´è®­ç»ƒepisodes
    if args.episodes is None:
        # ä½¿ç”¨æ•°æ®å¢å¼ºå¯ä»¥å¢åŠ 3-5å€çš„æœ‰æ•ˆepisodes
        if CONFIG.aug.USE_AUGMENTATION:
            args.episodes = min(available_episodes * 3, CONFIG.train.NUM_EPISODES)
            print(f"âœ“ æ•°æ®å¢å¼ºå·²å¯ç”¨ï¼Œè°ƒæ•´è®­ç»ƒepisodesåˆ°: {args.episodes}")
        else:
            args.episodes = min(available_episodes, CONFIG.train.NUM_EPISODES)
            print(f"âš  æ•°æ®å¢å¼ºæœªå¯ç”¨ï¼Œè®­ç»ƒepisodes: {args.episodes}")

    # â­ æ˜¾ç¤ºå°†è®­ç»ƒçš„ç®—æ³•
    print("\n" + "=" * 80)
    print("è®­ç»ƒé…ç½®".center(80))
    print("=" * 80)
    print(f"æ•°æ®æº: {'çœŸå®Excelæ•°æ® âœ“' if data_status == 'real' else 'æ¨¡æ‹Ÿæ•°æ® âš '}")
    print(f"ç®—æ³•æ•°é‡: {len(args.algorithms)}")
    print(f"è®­ç»ƒEpisodes: {args.episodes}")
    print(f"è¯„ä¼°Episodes: {args.eval_episodes}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"\nå°†è®­ç»ƒçš„ç®—æ³•:")
    for i, algo in enumerate(args.algorithms, 1):
        print(f"  {i}. {algo.upper()}")
    print("=" * 80)

    # ç¡®è®¤é…ç½®
    if not args.skip_confirmation:
        if not confirm_training_config(args.algorithms, args.episodes, data_status):
            print("\nè®­ç»ƒå·²å–æ¶ˆ")
            return

    # åˆ›å»ºç›®å½•
    os.makedirs(CONFIG.output.MODEL_DIR, exist_ok=True)
    os.makedirs(CONFIG.vis.RESULTS_DIR, exist_ok=True)

    # ========== è®­ç»ƒé˜¶æ®µ ==========
    if args.mode in ['train', 'full']:
        print("\n" + "=" * 80)
        print("å¼€å§‹å¤šç®—æ³•è®­ç»ƒ".center(80))
        print("=" * 80)

        trainer = MultiAlgorithmTrainer(data, CONFIG.train)
        training_results = trainer.train_all(args.algorithms, args.episodes)

        # ä¿å­˜è®­ç»ƒç»“æœ
        with open(os.path.join(CONFIG.vis.RESULTS_DIR, 'training_results.pkl'), 'wb') as f:
            pickle.dump(training_results, f)

        print("\nâœ“ è®­ç»ƒç»“æœå·²ä¿å­˜")
    else:
        training_results = None

    # ========== è¯„ä¼°é˜¶æ®µ ==========
    if args.mode in ['eval', 'full']:
        print("\n" + "=" * 80)
        print("å¼€å§‹ç®—æ³•è¯„ä¼°".center(80))
        print("=" * 80)

        # åŠ è½½è®­ç»ƒç»“æœ
        results_file = os.path.join(CONFIG.vis.RESULTS_DIR, 'training_results.pkl')
        if os.path.exists(results_file):
            with open(results_file, 'rb') as f:
                training_results = pickle.load(f)
        elif training_results is None:
            print("é”™è¯¯: æœªæ‰¾åˆ°è®­ç»ƒç»“æœ")
            if args.mode == 'eval':
                print("æç¤º: è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼ (--mode train)")
                return
            else:
                print("âš  è·³è¿‡è¯„ä¼°é˜¶æ®µ")
                training_results = {}

        # ç¡®ä¿training_resultsä¸æ˜¯None
        if training_results is None:
            training_results = {}

        # è¯„ä¼°
        evaluator = MultiAlgorithmEvaluator()

        for algo_name in training_results.keys():
            print(f"\nè¯„ä¼° {algo_name.upper()}...")

            # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
            env = ImprovedTransformerCoolingEnv(data)

            if algo_name == 'improved_sac':
                agent = ImprovedSAC(env.state_dim, env.action_dim)
            elif algo_name == 'sac':
                agent = BaseSAC(env.state_dim, env.action_dim)
            elif algo_name == 'ppo':
                agent = PPO(env.state_dim, env.action_dim)
            elif algo_name == 'ddpg':
                agent = DDPG(env.state_dim, env.action_dim)
            elif algo_name == 'td3':
                agent = TD3(env.state_dim, env.action_dim)
            else:
                print(f"  âš  è·³è¿‡æœªçŸ¥ç®—æ³•: {algo_name}")
                continue

            # åŠ è½½æœ€ä½³æ¨¡å‹
            model_path = os.path.join(CONFIG.output.MODEL_DIR, f"best_{algo_name}.pth")
            if os.path.exists(model_path):
                try:
                    agent.load_model(model_path)
                    print(f"  âœ“ å·²åŠ è½½æ¨¡å‹: {model_path}")

                    # è¯„ä¼°
                    eval_result = evaluator.evaluate_algorithm(
                        env, agent, algo_name, args.eval_episodes
                    )

                    print(f"  âœ“ è¯„ä¼°å®Œæˆ")
                    metrics = eval_result['metrics']
                    print(f"    å¹³å‡å›æŠ¥:  {eval_result['summary']['avg_reward']:.2f}")
                    print(f"    MAE:       {metrics.get('MAE', 0):.4f}Â°C")
                    print(f"    RMSE:      {metrics.get('RMSE', 0):.4f}Â°C")
                    print(f"    RÂ²:        {metrics.get('R2', 0):.4f}")
                except Exception as e:
                    print(f"  âœ— è¯„ä¼°å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"  âš  æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")

        # æ‰“å°è¯¦ç»†ç»“æœ
        if evaluator.results:
            evaluator.print_detailed_results()

            # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
            comparison_df = evaluator.compare_algorithms(save_table=True)
            print("\nç®—æ³•å¯¹æ¯”:")
            print(comparison_df.to_string(index=False))

            # ä¿å­˜è¯„ä¼°ç»“æœ
            evaluator.save_all_results()

            # ç”ŸæˆCSVå’Œè¡¨æ ¼
            generate_evaluation_csv_files(evaluator.results, 'results')
            generate_metrics_table(evaluator.results, 'tables')

    # æ‰“å°å®Œæˆä¿¡æ¯
    print("\n" + "=" * 80)
    print("âœ“ ç¨‹åºè¿è¡Œå®Œæˆ!".center(80))
    print("=" * 80)

    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - æ¨¡å‹: {CONFIG.output.MODEL_DIR}/")
    print(f"  - ç»“æœ: {CONFIG.vis.RESULTS_DIR}/")
    print(f"  - è¡¨æ ¼: {CONFIG.vis.TABLE_DIR}/")

    print("\nğŸ’¡ åç»­æ­¥éª¤:")
    print("  1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿: æ£€æŸ¥ results/ ç›®å½•")
    print("  2. åˆ†æå¯¹æ¯”è¡¨æ ¼: æŸ¥çœ‹ tables/ ç›®å½•")
    print("  3. æµ‹è¯•æœ€ä½³æ¨¡å‹: ä½¿ç”¨ --mode eval")


if __name__ == "__main__":
    main()