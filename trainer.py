"""
è®­ç»ƒæ¨¡å— - ä¿®å¤ç‰ˆï¼ˆç¡®ä¿ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡æ•°æ®ï¼‰

ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼š
1. âœ… ç®€åŒ–æå–é€»è¾‘ï¼šè®­ç»ƒç»“æŸæ—¶ä¸€æ¬¡æ€§å¤åˆ¶æ‰€æœ‰æ•°æ®
2. âœ… ä¸å†ä½¿ç”¨å¢é‡ä¿å­˜ï¼ˆé¿å…ç´¢å¼•é”™è¯¯ï¼‰
3. âœ… æ·»åŠ è°ƒè¯•ä¿¡æ¯éªŒè¯æå–ç»“æœ
4. âœ… ç¡®ä¿actor_losses, critic_losses, entropies, alphaséƒ½è¢«ä¿å­˜
"""

import numpy as np
import pandas as pd
import torch
from typing import Dict, List, Tuple
import os
from tqdm import tqdm
import pickle

from environment import ImprovedTransformerCoolingEnv
from sac_base import BaseSAC
from sac_temperature_aware import ImprovedSAC
from ppo import PPO
from ddpg import DDPG
from td3 import TD3
from metrics import MetricsCalculator
from config import CONFIG


class Trainer:
    """è®­ç»ƒå™¨ï¼ˆä¿®å¤ç‰ˆ - ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡ï¼‰"""

    def __init__(self, env: ImprovedTransformerCoolingEnv, agent, algorithm_name: str):
        self.env = env
        self.agent = agent
        self.algorithm_name = algorithm_name

        # ä½¿ç”¨metrics.pyä¸­çš„è®¡ç®—å™¨
        self.metrics_calculator = MetricsCalculator()

        # è®­ç»ƒè®°å½•
        self.episode_rewards = []
        self.episode_lengths = []
        self.eval_metrics_history = []

        # ğŸ”¥ è®­ç»ƒç»Ÿè®¡ï¼ˆä»agentæå–ï¼‰
        self.training_stats = {
            'actor_losses': [],
            'critic_losses': [],
            'entropies': [],
            'alphas': [],
            'training_steps': []
        }

        self.training_data = {
            'rewards': [],
            'temperatures': [],
            'actions': [],
            'losses': [],
            'cooling_data': []
        }

    def _extract_agent_stats(self):
        """
        ğŸ”¥ æ ¸å¿ƒæ–¹æ³•ï¼šä»agentæå–å½“å‰è®­ç»ƒç»Ÿè®¡å¹¶ä¿å­˜

        è®­ç»ƒç»“æŸæ—¶è°ƒç”¨ä¸€æ¬¡ï¼Œç›´æ¥å¤åˆ¶æ‰€æœ‰æ•°æ®
        """
        try:
            if not hasattr(self.agent, 'get_training_stats'):
                print(f"  âš  {self.algorithm_name} æ²¡æœ‰get_training_stats()æ–¹æ³•")
                return

            current_stats = self.agent.get_training_stats()

            if not isinstance(current_stats, dict):
                print(f"  âš  get_training_stats()è¿”å›çš„ä¸æ˜¯å­—å…¸: {type(current_stats)}")
                return

            # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥å¤åˆ¶æ‰€æœ‰æ•°æ®
            if 'actor_losses' in current_stats:
                actor_losses = current_stats['actor_losses']
                if isinstance(actor_losses, list) and len(actor_losses) > 0:
                    self.training_stats['actor_losses'] = list(actor_losses)

            if 'critic_losses' in current_stats:
                critic_losses = current_stats['critic_losses']
                if isinstance(critic_losses, list) and len(critic_losses) > 0:
                    self.training_stats['critic_losses'] = list(critic_losses)

            # SACç‰¹æœ‰ï¼šç†µå’Œalpha
            if 'entropies' in current_stats:
                entropies = current_stats['entropies']
                if isinstance(entropies, list) and len(entropies) > 0:
                    self.training_stats['entropies'] = list(entropies)

            if 'alphas' in current_stats:
                alphas = current_stats['alphas']
                if isinstance(alphas, list) and len(alphas) > 0:
                    self.training_stats['alphas'] = list(alphas)

            # ğŸ”¥ æ–°å¢ï¼šå¯¹äºæ²¡æœ‰alphasåˆ—è¡¨çš„ç®—æ³•ï¼Œå°è¯•ç”Ÿæˆå ä½æ•°æ®
            if len(self.training_stats['alphas']) == 0:
                # å¦‚æœæœ‰critic_lossesï¼Œç”Ÿæˆç›¸åŒé•¿åº¦çš„å ä½æ•°æ®
                if len(self.training_stats['critic_losses']) > 0:
                    # ä½¿ç”¨é»˜è®¤alphaå€¼0.2å¡«å……
                    placeholder_alpha = 0.2
                    if hasattr(self.agent, 'log_alpha'):
                        placeholder_alpha = self.agent.log_alpha.exp().item()
                    self.training_stats['alphas'] = [placeholder_alpha] * len(self.training_stats['critic_losses'])

            # è®°å½•è®­ç»ƒæ­¥æ•°
            if 'training_step' in current_stats:
                self.training_stats['training_steps'].append(
                    current_stats['training_step']
                )

        except Exception as e:
            print(f"  âš  æå–è®­ç»ƒç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def train_episode(self) -> Tuple[float, int, Dict]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = self.env.reset()
        episode_reward = 0
        episode_length = 0
        episode_temps = []
        episode_actions = []
        episode_coolings = []

        done = False
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm_name == 'ppo':
                action, log_prob, value = self.agent.select_action(state)
            else:
                action = self.agent.select_action(state, evaluate=False)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)

            # å­˜å‚¨è½¬ç§»
            if self.algorithm_name == 'ppo':
                self.agent.store_transition(state, action, reward, log_prob, value, done)
            else:
                self.agent.store_transition(state, action, reward, next_state, done)

            # è®°å½•
            episode_temps.append(info['oil_temp'])
            episode_actions.append(action.copy())
            episode_coolings.append({
                'actual': info.get('actual_cooling', 0),
                'target': info.get('target_cooling', 0),
                'error': info.get('cooling_error', 0)
            })
            episode_reward += reward
            episode_length += 1

            state = next_state

        info = {
            'temperatures': episode_temps,
            'actions': np.array(episode_actions),
            'coolings': episode_coolings
        }

        return episode_reward, episode_length, info

    def evaluate(self, num_episodes: int = None) -> Dict:
        """è¯„ä¼°æ™ºèƒ½ä½“"""
        if num_episodes is None:
            num_episodes = CONFIG.train.EVAL_EPISODES

        eval_rewards = []
        all_temperatures = []
        all_actions = []
        all_episode_rewards = []
        all_actual_coolings = []
        all_target_coolings = []

        for _ in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            episode_temps = []
            episode_actions = []
            episode_actual_coolings = []
            episode_target_coolings = []
            done = False

            while not done:
                if self.algorithm_name == 'ppo':
                    action, log_prob, _ = self.agent.select_action(state, evaluate=True)
                else:
                    action = self.agent.select_action(state, evaluate=True)

                next_state, reward, done, info = self.env.step(action)

                episode_reward += reward
                episode_temps.append(info['oil_temp'])
                episode_actions.append(action.copy())
                episode_actual_coolings.append(info.get('actual_cooling', 0))
                episode_target_coolings.append(info.get('target_cooling', 0))

                state = next_state

            eval_rewards.append(episode_reward)
            all_episode_rewards.append(episode_reward)
            all_temperatures.extend(episode_temps)
            all_actions.extend(episode_actions)
            all_actual_coolings.extend(episode_actual_coolings)
            all_target_coolings.extend(episode_target_coolings)

        all_temperatures = np.array(all_temperatures)
        all_actions = np.array(all_actions)
        all_actual_coolings = np.array(all_actual_coolings)
        all_target_coolings = np.array(all_target_coolings)

        try:
            all_metrics = self.metrics_calculator.calculate_all_metrics(
                temperatures=all_temperatures,
                rewards=all_episode_rewards,
                actions=all_actions,
                actual_coolings=all_actual_coolings,
                target_coolings=all_target_coolings
            )
        except Exception as e:
            print(f"  âš  å®Œæ•´æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            all_metrics = {
                'cooling_mae': np.mean(np.abs(all_actual_coolings - all_target_coolings)),
                'avg_reward': np.mean(all_episode_rewards)
            }

        return all_metrics

    def train(self, num_episodes: int = None, eval_interval: int = None) -> Dict:
        """è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆä¿®å¤ç‰ˆ - è®­ç»ƒç»“æŸåæå–ç»Ÿè®¡ï¼‰"""
        if num_episodes is None:
            num_episodes = CONFIG.train.NUM_EPISODES
        if eval_interval is None:
            eval_interval = CONFIG.train.EVAL_FREQUENCY

        print(f"\nå¼€å§‹è®­ç»ƒ {self.algorithm_name}...")
        print(f"è®­ç»ƒEpisodes: {num_episodes}")
        print(f"è¯„ä¼°é—´éš”: {eval_interval} episodes")
        print(f"ğŸ”¥ è¯„ä»·ä½“ç³»: é™æ¸©èƒ½åŠ›ï¼ˆä¸»è¦æŒ‡æ ‡: {CONFIG.metrics.BEST_MODEL_CRITERION}ï¼‰")

        best_eval_reward = -np.inf
        best_cooling_mae = np.inf
        best_model_saved = False

        for episode in tqdm(range(num_episodes), desc=f"è®­ç»ƒ {self.algorithm_name}"):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_reward, episode_length, info = self.train_episode()

            # æ›´æ–°ç½‘ç»œ
            if self.algorithm_name == 'ppo':
                self.agent.update()
            else:
                for _ in range(episode_length):
                    self.agent.update()

            # è®°å½•
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_length)
            self.training_data['rewards'].append(episode_reward)
            self.training_data['temperatures'].extend(info['temperatures'])
            self.training_data['actions'].append(info['actions'])
            self.training_data['cooling_data'].append(info['coolings'])

            # å®šæœŸè¯„ä¼°
            if (episode + 1) % eval_interval == 0:
                try:
                    eval_metrics = self.evaluate()
                    self.eval_metrics_history.append(eval_metrics)

                    current_metric = self.metrics_calculator.get_best_metric_value(eval_metrics)
                    cooling_mae = eval_metrics.get('cooling_mae', np.inf)
                    cooling_precision_1c = eval_metrics.get('cooling_precision_1c', 0.0)
                    cooling_precision_2c = eval_metrics.get('cooling_precision_2c', 0.0)
                    total_cooling = eval_metrics.get('total_cooling', 0.0)
                    eval_reward = eval_metrics.get('avg_reward', -np.inf)

                    is_best = current_metric < best_cooling_mae

                    if is_best:
                        best_cooling_mae = current_metric
                        best_eval_reward = eval_reward
                        self.save_model(f"best_{self.algorithm_name}.pth")
                        best_model_saved = True

                    print(f"\n{'=' * 100}")
                    print(f"Episode {episode + 1}/{num_episodes} - {self.algorithm_name.upper()}")
                    print(f"{'=' * 100}")

                    print(f"\nğŸ”¥ğŸ”¥ğŸ”¥ é™æ¸©èƒ½åŠ›æŒ‡æ ‡ï¼ˆæ ¸å¿ƒè¯„ä»·ï¼‰:")
                    print(
                        f"  {CONFIG.metrics.BEST_MODEL_CRITERION}:  {current_metric:8.4f}Â°C {'â­ æ–°æœ€ä½³!' if is_best else ''}")
                    print(f"  é™æ¸©ç²¾åº¦(Â±1Â°C):         {cooling_precision_1c:8.2f}%")
                    print(f"  é™æ¸©ç²¾åº¦(Â±2Â°C):         {cooling_precision_2c:8.2f}%")
                    print(f"  æ€»é™æ¸©é‡:                {total_cooling:8.2f}Â°C")

                    print(f"\nğŸ’° å¼ºåŒ–å­¦ä¹ æŒ‡æ ‡:")
                    print(f"  è®­ç»ƒå›æŠ¥:                {episode_reward:8.2f}")
                    print(f"  è¯„ä¼°å›æŠ¥:                {eval_reward:8.2f}")

                    print(f"\nğŸ“ˆ æœ€ä½³è®°å½•:")
                    print(f"  æœ€ä½³{CONFIG.metrics.BEST_MODEL_CRITERION}: {best_cooling_mae:8.4f}Â°C")
                    print(f"  æœ€ä½³å›æŠ¥:                {best_eval_reward:8.2f}")

                    print(f"{'=' * 100}\n")

                except Exception as e:
                    print(f"\nâš  è¯„ä¼°å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        # ğŸ”¥ è®­ç»ƒç»“æŸï¼Œæå–è®­ç»ƒç»Ÿè®¡
        print("\næ­£åœ¨æå–è®­ç»ƒç»Ÿè®¡æ•°æ®...")
        self._extract_agent_stats()

        # ğŸ”¥ éªŒè¯æå–ç»“æœ
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡æå–ç»“æœ:")
        print(f"  ActoræŸå¤±:   {len(self.training_stats['actor_losses'])} æ¡")
        print(f"  CriticæŸå¤±:  {len(self.training_stats['critic_losses'])} æ¡")
        print(f"  ç†µ:          {len(self.training_stats['entropies'])} æ¡")
        print(f"  Alpha:       {len(self.training_stats['alphas'])} æ¡")

        if len(self.training_stats['actor_losses']) == 0:
            print(f"  âš ï¸ è­¦å‘Šï¼šæœªèƒ½æå–åˆ°actor_lossesï¼")
            print(f"     æ£€æŸ¥{self.algorithm_name}.get_training_stats()æ˜¯å¦æ­£ç¡®å®ç°")

        print(f"\nâœ“ {self.algorithm_name} è®­ç»ƒå®Œæˆ!")
        print(f"  å®ŒæˆEpisodes: {num_episodes}")
        print(f"  ğŸ”¥ æœ€ä½³{CONFIG.metrics.BEST_MODEL_CRITERION}: {best_cooling_mae:.4f}Â°C")
        print(f"  æœ€ä½³å›æŠ¥: {best_eval_reward:.2f}")

        if best_model_saved:
            print(f"  âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")

        # ğŸ”¥ ä¿å­˜è®­ç»ƒç»“æœï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒç»Ÿè®¡ï¼‰
        results = {
            'algorithm': self.algorithm_name,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'eval_metrics_history': self.eval_metrics_history,
            'training_data': self.training_data,
            'best_reward': best_eval_reward,
            'best_cooling_mae': best_cooling_mae,
            'final_metrics': self.eval_metrics_history[-1] if self.eval_metrics_history else {},

            # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒï¼šä¿å­˜è®­ç»ƒç»Ÿè®¡
            'training_stats': self.training_stats,

            'config': {
                'num_episodes': num_episodes,
                'eval_frequency': eval_interval,
                'eval_episodes': CONFIG.train.EVAL_EPISODES,
                'best_criterion': CONFIG.metrics.BEST_MODEL_CRITERION,
            }
        }

        return results

    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(CONFIG.output.MODEL_DIR, exist_ok=True)
        filepath = os.path.join(CONFIG.output.MODEL_DIR, filename)
        self.agent.save_model(filepath)

    def load_model(self, filename: str):
        """åŠ è½½æ¨¡å‹"""
        filepath = os.path.join(CONFIG.output.MODEL_DIR, filename)
        self.agent.load_model(filepath)


class MultiAlgorithmTrainer:
    """å¤šç®—æ³•è®­ç»ƒç®¡ç†å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, env_data: pd.DataFrame):
        self.env_data = env_data
        self.results = {}

    def create_agent(self, algorithm: str, state_dim: int, action_dim: int):
        """åˆ›å»ºæ™ºèƒ½ä½“"""
        if algorithm == 'improved_sac':
            return ImprovedSAC(state_dim, action_dim)
        elif algorithm == 'sac':
            return BaseSAC(state_dim, action_dim)
        elif algorithm == 'ppo':
            return PPO(state_dim, action_dim)
        elif algorithm == 'ddpg':
            return DDPG(state_dim, action_dim)
        elif algorithm == 'td3':
            return TD3(state_dim, action_dim)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

    def train_algorithm(self, algorithm: str, num_episodes: int = None) -> Dict:
        """è®­ç»ƒå•ä¸ªç®—æ³•"""
        env = ImprovedTransformerCoolingEnv(self.env_data)
        state_dim = env.state_dim
        action_dim = env.action_dim

        agent = self.create_agent(algorithm, state_dim, action_dim)
        trainer = Trainer(env, agent, algorithm)

        results = trainer.train(num_episodes=num_episodes)
        self.results[algorithm] = results

        return results

    def train_all(self, algorithms: List[str] = None, num_episodes: int = None):
        """è®­ç»ƒæ‰€æœ‰ç®—æ³•"""
        if algorithms is None:
            algorithms = CONFIG.algo.ALGORITHMS

        if num_episodes is None:
            num_episodes = CONFIG.train.NUM_EPISODES

        print("=" * 100)
        print("å¼€å§‹å¤šç®—æ³•è®­ç»ƒï¼ˆä¿®å¤ç‰ˆ - ç¡®ä¿æå–training_statsï¼‰")
        print("=" * 100)
        print(f"ç®—æ³•åˆ—è¡¨: {algorithms}")
        print(f"è®­ç»ƒEpisodes: {num_episodes}")
        print("=" * 100)

        for algo in algorithms:
            print(f"\n{'=' * 100}")
            print(f"è®­ç»ƒç®—æ³•: {algo.upper()}")
            print(f"{'=' * 100}")

            try:
                results = self.train_algorithm(algo, num_episodes)
                print(f"\nâœ“ {algo} è®­ç»ƒæˆåŠŸ")

                final_metrics = results.get('final_metrics', {})
                print(f"\næœ€ç»ˆè¯„ä¼°æŒ‡æ ‡:")
                print(f"  ğŸ”¥ {CONFIG.metrics.BEST_MODEL_CRITERION}: {final_metrics.get('cooling_mae', 0):.4f}Â°C")

                # ğŸ”¥ æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡æ‘˜è¦
                training_stats = results.get('training_stats', {})
                print(f"\nè®­ç»ƒç»Ÿè®¡æ‘˜è¦:")
                print(f"  ActoræŸå¤±: {len(training_stats.get('actor_losses', []))} æ¡è®°å½•")
                print(f"  CriticæŸå¤±: {len(training_stats.get('critic_losses', []))} æ¡è®°å½•")
                if training_stats.get('entropies'):
                    print(f"  ç†µ: {len(training_stats['entropies'])} æ¡è®°å½•")
                if training_stats.get('alphas'):
                    print(f"  Alpha: {len(training_stats['alphas'])} æ¡è®°å½•")

            except Exception as e:
                print(f"\nâœ— {algo} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        print("\n" + "=" * 100)
        print("æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ!")
        print("=" * 100)

        self._print_cooling_comparison()
        self.save_all_results()

    def _print_cooling_comparison(self):
        """æ‰“å°é™æ¸©èƒ½åŠ›å¯¹æ¯”"""
        if not self.results:
            return

        print("\n" + "=" * 100)
        print("ç®—æ³•å¯¹æ¯”ï¼ˆåŸºäºé™æ¸©èƒ½åŠ›ï¼‰")
        print("=" * 100)

        criterion = CONFIG.metrics.BEST_MODEL_CRITERION

        print(f"{'ç®—æ³•':<15} | {criterion:>12} | {'ç²¾åº¦Â±1Â°C':>10} | {'ç²¾åº¦Â±2Â°C':>10} | "
              f"{'æ€»é™æ¸©':>10} | {'å›æŠ¥':>10} | {'Episodes':>10}")
        print("-" * 100)

        for algo, results in self.results.items():
            final_metrics = results.get('final_metrics', {})
            config_info = results.get('config', {})

            print(f"{algo.upper():<15} | "
                  f"{final_metrics.get('cooling_mae', 0):>12.4f} | "
                  f"{final_metrics.get('cooling_precision_1c', 0):>10.2f}% | "
                  f"{final_metrics.get('cooling_precision_2c', 0):>10.2f}% | "
                  f"{final_metrics.get('total_cooling', 0):>10.2f} | "
                  f"{final_metrics.get('avg_reward', 0):>10.2f} | "
                  f"{config_info.get('num_episodes', len(results['episode_rewards'])):>10}")

        print("=" * 100)

    def save_all_results(self):
        """ä¿å­˜æ‰€æœ‰è®­ç»ƒç»“æœï¼ˆåŒ…å«è®­ç»ƒç»Ÿè®¡ï¼‰"""
        os.makedirs(CONFIG.vis.RESULTS_DIR, exist_ok=True)
        filepath = os.path.join(CONFIG.vis.RESULTS_DIR, 'training_results_fixed.pkl')

        save_data = {
            'results': self.results,
            'config_snapshot': {
                'num_episodes': CONFIG.train.NUM_EPISODES,
                'eval_frequency': CONFIG.train.EVAL_FREQUENCY,
                'eval_episodes': CONFIG.train.EVAL_EPISODES,
                'best_criterion': CONFIG.metrics.BEST_MODEL_CRITERION,
            }
        }

        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)

        print(f"\nâœ“ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        print("  ğŸ”¥ åŒ…å«å®Œæ•´çš„training_statsæ•°æ®")

        # ğŸ”¥ éªŒè¯ä¿å­˜çš„æ•°æ®
        print("\nğŸ“‹ éªŒè¯ä¿å­˜çš„training_stats:")
        for algo, result in self.results.items():
            stats = result.get('training_stats', {})
            print(f"  {algo}:")
            print(f"    ActoræŸå¤±: {len(stats.get('actor_losses', []))} æ¡")
            print(f"    CriticæŸå¤±: {len(stats.get('critic_losses', []))} æ¡")
            print(f"    ç†µ: {len(stats.get('entropies', []))} æ¡")
            print(f"    Alpha: {len(stats.get('alphas', []))} æ¡")


if __name__ == "__main__":
    print("=" * 90)
    print("ä¿®å¤ç‰ˆè®­ç»ƒæ¨¡å— - ç¡®ä¿ä¿å­˜å®Œæ•´è®­ç»ƒç»Ÿè®¡".center(90))
    print("=" * 90)

    print("\nğŸ”¥ æ ¸å¿ƒä¿®å¤:")
    print("  1. âœ… ç®€åŒ–æå–é€»è¾‘ï¼šè®­ç»ƒç»“æŸæ—¶ä¸€æ¬¡æ€§å¤åˆ¶æ‰€æœ‰æ•°æ®")
    print("  2. âœ… ä¸å†ä½¿ç”¨å¢é‡ä¿å­˜ï¼ˆé¿å…ç´¢å¼•é”™è¯¯ï¼‰")
    print("  3. âœ… æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å’ŒéªŒè¯")
    print("  4. âœ… å¯¹äºç¼ºå¤±çš„alphasï¼Œç”Ÿæˆå ä½æ•°æ®")
    print("  5. âœ… ä¿å­˜åç«‹å³éªŒè¯training_statså†…å®¹")

    print("\nğŸ“Š æ•°æ®æµ:")
    print("  è®­ç»ƒå®Œæˆ â†’ _extract_agent_stats() â†’ ç›´æ¥å¤åˆ¶agentæ•°æ®")
    print("  â†’ ä¿å­˜åˆ°results['training_stats'] â†’ éªŒè¯æ•°æ®é•¿åº¦")

    print("\n" + "=" * 90)