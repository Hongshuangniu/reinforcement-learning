"""
10kVå˜å‹å™¨æ™ºèƒ½å†·å´æ§åˆ¶ç³»ç»Ÿ - è®­ç»ƒæ¨¡å—ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰
ç»Ÿä¸€çš„è®­ç»ƒæ¥å£ï¼Œæ”¯æŒæ‰€æœ‰ç®—æ³•

å…³é”®æ”¹è¿›ï¼š
1. âœ… åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å®Œæ•´æŒ‡æ ‡
2. âœ… æ˜¾ç¤ºæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼ˆä¸ä»…æ˜¯MAEï¼‰
3. âœ… ä¿å­˜è¯¦ç»†çš„è®­ç»ƒç»Ÿè®¡
4. âœ… åŸºäºå¤šä¸ªæŒ‡æ ‡ç»¼åˆåˆ¤æ–­æ¨¡å‹ä¼˜åŠ£
5. âœ… åˆ é™¤æ—©åœæœºåˆ¶ï¼Œå®Œæ•´è®­ç»ƒæ‰€æœ‰episodes
"""

import numpy as np
import pandas as pd
import torch
from typing import Dict, List, Tuple
import os
from tqdm import tqdm
import pickle

from environment import ImprovedTransformerCoolingEnv
from sac_base import BaseSAC
from sac_temperature_aware import ImprovedSAC
from ppo import PPO
from ddpg import DDPG
from td3 import TD3
from metrics import MetricsCalculator
from config import CONFIG, TrainingConfig


class Trainer:
    """è®­ç»ƒå™¨åŸºç±»ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰"""

    def __init__(self, env: ImprovedTransformerCoolingEnv, agent, algorithm_name: str,
                 config: TrainingConfig = TrainingConfig()):
        self.env = env
        self.agent = agent
        self.algorithm_name = algorithm_name
        self.config = config

        # â­ æ·»åŠ æŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = MetricsCalculator()

        # è®­ç»ƒè®°å½•
        self.episode_rewards = []
        self.episode_lengths = []
        self.eval_rewards = []
        self.eval_metrics_history = []  # â­ ä¿å­˜æ¯æ¬¡è¯„ä¼°çš„å®Œæ•´æŒ‡æ ‡

        self.training_data = {
            'rewards': [],
            'temperatures': [],
            'actions': [],
            'losses': []
        }

    def train_episode(self) -> Tuple[float, int, Dict]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = self.env.reset()
        episode_reward = 0
        episode_length = 0
        episode_temps = []
        episode_actions = []

        done = False
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm_name == 'ppo':
                action, log_prob, value = self.agent.select_action(state)
            else:
                action = self.agent.select_action(state, evaluate=False)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)

            # å­˜å‚¨è½¬ç§»
            if self.algorithm_name == 'ppo':
                self.agent.store_transition(state, action, reward, log_prob, value, done)
            else:
                self.agent.store_transition(state, action, reward, next_state, done)

            # è®°å½•
            episode_temps.append(info['oil_temp'])
            episode_actions.append(action.copy())
            episode_reward += reward
            episode_length += 1

            state = next_state

        info = {
            'temperatures': episode_temps,
            'actions': np.array(episode_actions)
        }

        return episode_reward, episode_length, info

    def evaluate(self, num_episodes: int = 10) -> Dict:
        """
        è¯„ä¼°æ™ºèƒ½ä½“ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆï¼‰

        Args:
            num_episodes: è¯„ä¼°episodeæ•°é‡

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        eval_rewards = []
        all_true_temps = []
        all_target_temps = []
        all_actions = []
        all_episode_rewards = []

        for _ in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            episode_temps = []
            episode_actions = []
            target_temps = []
            done = False

            while not done:
                # â­ è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                if self.algorithm_name == 'ppo':
                    action, _, _ = self.agent.select_action(state, evaluate=True)
                else:
                    action = self.agent.select_action(state, evaluate=True)

                next_state, reward, done, info = self.env.step(action)

                episode_reward += reward
                episode_temps.append(info['oil_temp'])
                target_temps.append(self.env.target_temp)
                episode_actions.append(action.copy())

                state = next_state

            eval_rewards.append(episode_reward)
            all_episode_rewards.append(episode_reward)
            all_true_temps.extend(episode_temps)
            all_target_temps.extend(target_temps)
            all_actions.extend(episode_actions)

        # â­ è®¡ç®—å®Œæ•´çš„æ§åˆ¶æ€§èƒ½æŒ‡æ ‡
        all_true_temps = np.array(all_true_temps)
        all_target_temps = np.array(all_target_temps)
        all_actions = np.array(all_actions)

        control_metrics = self.metrics_calculator.calculate_control_metrics(
            y_true=all_true_temps,
            y_pred=all_target_temps
        )

        # â­ è®¡ç®—å®Œæ•´çš„RLæŒ‡æ ‡
        rl_metrics = self.metrics_calculator.calculate_rl_metrics(
            rewards=all_episode_rewards,
            actions=all_actions,
            temperatures=all_true_temps
        )

        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            **control_metrics,
            **rl_metrics,
            'mean_eval_reward': np.mean(eval_rewards),
            'std_eval_reward': np.std(eval_rewards),
            'mean_temp': np.mean(all_true_temps),
            'std_temp': np.std(all_true_temps)
        }

        return all_metrics

    def train(self, num_episodes: int, eval_interval: int = 10) -> Dict:
        """
        è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰

        å…³é”®æ”¹è¿›ï¼š
        1. è®¡ç®—å¹¶æ˜¾ç¤ºæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        2. åŸºäºç»¼åˆæŒ‡æ ‡ä¿å­˜æœ€ä½³æ¨¡å‹
        3. åˆ é™¤æ—©åœæœºåˆ¶ï¼Œå®Œæ•´è®­ç»ƒæ‰€æœ‰episodes
        """
        print(f"\nå¼€å§‹è®­ç»ƒ {self.algorithm_name}...")
        print(f"è®­ç»ƒEpisodes: {num_episodes}")
        print(f"è¯„ä¼°é—´éš”: {eval_interval} episodes")

        best_eval_reward = -np.inf
        best_mae = np.inf
        best_rmse = np.inf
        epochs_since_improvement = 0

        for episode in tqdm(range(num_episodes), desc=f"è®­ç»ƒ {self.algorithm_name}"):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_reward, episode_length, info = self.train_episode()

            # æ›´æ–°ç½‘ç»œ
            if self.algorithm_name == 'ppo':
                self.agent.update()
            else:
                for _ in range(episode_length):
                    self.agent.update()

            # è®°å½•
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_length)
            self.training_data['rewards'].append(episode_reward)
            self.training_data['temperatures'].extend(info['temperatures'])
            self.training_data['actions'].append(info['actions'])

            # â­ è¯„ä¼°
            if (episode + 1) % eval_interval == 0:
                eval_metrics = self.evaluate(num_episodes=10)
                self.eval_metrics_history.append(eval_metrics)

                # æå–å…³é”®æŒ‡æ ‡
                eval_reward = eval_metrics['mean_eval_reward']
                eval_mae = eval_metrics['MAE']
                eval_rmse = eval_metrics['RMSE']
                eval_mape = eval_metrics['MAPE']
                eval_r2 = eval_metrics['R2']
                eval_max_ae = eval_metrics['MaxAE']

                # â­ åŸºäºMAEåˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best = False
                if eval_mae < best_mae:
                    best_mae = eval_mae
                    best_rmse = eval_rmse
                    best_eval_reward = eval_reward
                    self.save_model(f"best_{self.algorithm_name}.pth")
                    epochs_since_improvement = 0
                    is_best = True
                else:
                    epochs_since_improvement += 1

                # â­ æ˜¾ç¤ºå®Œæ•´æŒ‡æ ‡
                print(f"\nEpisode {episode + 1}/{num_episodes}")
                print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                print(f"ğŸ“Š æ§åˆ¶æ€§èƒ½æŒ‡æ ‡:")
                print(f"  MAE:   {eval_mae:8.4f}Â°C {'â­ æ–°æœ€ä½³!' if is_best else ''}")
                print(f"  RMSE:  {eval_rmse:8.4f}Â°C")
                print(f"  MAPE:  {eval_mape:8.4f}%")
                print(f"  RÂ²:    {eval_r2:8.4f}")
                print(f"  MaxAE: {eval_max_ae:8.4f}Â°C")

                print(f"\nğŸ¯ å¼ºåŒ–å­¦ä¹ æŒ‡æ ‡:")
                print(f"  è®­ç»ƒå›æŠ¥: {episode_reward:8.2f}")
                print(f"  è¯„ä¼°å›æŠ¥: {eval_reward:8.2f} Â± {eval_metrics['std_eval_reward']:6.2f}")
                print(f"  å›æŠ¥æ–¹å·®: {eval_metrics['reward_variance']:8.4f}")

                if 'action_smoothness' in eval_metrics:
                    print(f"  åŠ¨ä½œå¹³æ»‘: {eval_metrics['action_smoothness']:8.4f}")
                if 'temp_smoothness' in eval_metrics:
                    print(f"  æ¸©åº¦å¹³æ»‘: {eval_metrics['temp_smoothness']:8.4f}")

                print(f"\nğŸ“ˆ æœ€ä½³è®°å½•:")
                print(f"  æœ€ä½³MAE:  {best_mae:8.4f}Â°C")
                print(f"  æœ€ä½³RMSE: {best_rmse:8.4f}Â°C")
                print(f"  æœ€ä½³å›æŠ¥: {best_eval_reward:8.2f}")

                # æ˜¾ç¤ºè·ç¦»ä¸Šæ¬¡æ”¹è¿›çš„è½®æ•°
                if epochs_since_improvement > 0:
                    print(f"\nğŸ’¡ è®­ç»ƒä¿¡æ¯:")
                    print(f"  è·ä¸Šæ¬¡æ”¹è¿›: {epochs_since_improvement} è½®è¯„ä¼°")

                print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        print(f"\nâœ“ {self.algorithm_name} è®­ç»ƒå®Œæˆ!")
        print(f"  å®ŒæˆEpisodes: {num_episodes}")
        print(f"  æœ€ä½³MAE:   {best_mae:.4f}Â°C")
        print(f"  æœ€ä½³RMSE:  {best_rmse:.4f}Â°C")
        print(f"  æœ€ä½³å›æŠ¥:  {best_eval_reward:.2f}")

        # ä¿å­˜è®­ç»ƒç»“æœ
        results = {
            'algorithm': self.algorithm_name,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'eval_rewards': self.eval_rewards,
            'eval_metrics_history': self.eval_metrics_history,  # â­ ä¿å­˜å®Œæ•´æŒ‡æ ‡å†å²
            'training_data': self.training_data,
            'best_reward': best_eval_reward,
            'best_mae': best_mae,
            'best_rmse': best_rmse,
            # â­ æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡
            'final_metrics': self.eval_metrics_history[-1] if self.eval_metrics_history else {}
        }

        return results

    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        filepath = os.path.join(CONFIG.output.MODEL_DIR, filename)
        self.agent.save_model(filepath)

    def load_model(self, filename: str):
        """åŠ è½½æ¨¡å‹"""
        filepath = os.path.join(CONFIG.output.MODEL_DIR, filename)
        self.agent.load_model(filepath)


class MultiAlgorithmTrainer:
    """å¤šç®—æ³•è®­ç»ƒç®¡ç†å™¨ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰"""

    def __init__(self, env_data: pd.DataFrame, config: TrainingConfig = TrainingConfig()):
        self.env_data = env_data
        self.config = config
        self.results = {}

    def create_agent(self, algorithm: str, state_dim: int, action_dim: int):
        """åˆ›å»ºæ™ºèƒ½ä½“"""
        if algorithm == 'improved_sac':
            return ImprovedSAC(state_dim, action_dim)
        elif algorithm == 'sac':
            return BaseSAC(state_dim, action_dim)
        elif algorithm == 'ppo':
            return PPO(state_dim, action_dim)
        elif algorithm == 'ddpg':
            return DDPG(state_dim, action_dim)
        elif algorithm == 'td3':
            return TD3(state_dim, action_dim)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

    def train_algorithm(self, algorithm: str, num_episodes: int = None) -> Dict:
        """è®­ç»ƒå•ä¸ªç®—æ³•"""
        if num_episodes is None:
            num_episodes = self.config.NUM_EPISODES

        # åˆ›å»ºç¯å¢ƒ
        env = ImprovedTransformerCoolingEnv(self.env_data)
        state_dim = env.state_dim
        action_dim = env.action_dim

        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = self.create_agent(algorithm, state_dim, action_dim)

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(env, agent, algorithm, self.config)

        # è®­ç»ƒ
        results = trainer.train(num_episodes, self.config.EVAL_FREQUENCY)

        # ä¿å­˜ç»“æœ
        self.results[algorithm] = results

        return results

    def train_all(self, algorithms: List[str], num_episodes: int = None):
        """è®­ç»ƒæ‰€æœ‰ç®—æ³•"""
        print("=" * 70)
        print("å¼€å§‹å¤šç®—æ³•è®­ç»ƒï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰")
        print("=" * 70)
        print("æ³¨æ„: æ‰€æœ‰ç®—æ³•å°†å®Œæ•´è®­ç»ƒæ‰€æœ‰episodes")
        print("=" * 70)

        for algo in algorithms:
            print(f"\n{'=' * 70}")
            print(f"è®­ç»ƒç®—æ³•: {algo.upper()}")
            print(f"{'=' * 70}")

            try:
                results = self.train_algorithm(algo, num_episodes)
                print(f"\nâœ“ {algo} è®­ç»ƒæˆåŠŸ")

                # â­ æ˜¾ç¤ºæœ€ç»ˆæŒ‡æ ‡
                final_metrics = results.get('final_metrics', {})
                print(f"\næœ€ç»ˆè¯„ä¼°æŒ‡æ ‡:")
                print(f"  MAE:   {final_metrics.get('MAE', 0):.4f}Â°C")
                print(f"  RMSE:  {final_metrics.get('RMSE', 0):.4f}Â°C")
                print(f"  MAPE:  {final_metrics.get('MAPE', 0):.4f}%")
                print(f"  RÂ²:    {final_metrics.get('R2', 0):.4f}")
                print(f"  å›æŠ¥:  {final_metrics.get('mean_eval_reward', 0):.2f}")

            except Exception as e:
                print(f"\nâœ— {algo} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        print("\n" + "=" * 70)
        print("æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ!")
        print("=" * 70)

        # æ‰“å°å¯¹æ¯”
        self._print_comparison()

        # ä¿å­˜æ‰€æœ‰ç»“æœ
        self.save_all_results()

    def _print_comparison(self):
        """æ‰“å°ç®—æ³•å¯¹æ¯”ï¼ˆå®Œæ•´æŒ‡æ ‡ï¼‰"""
        if not self.results:
            return

        print("\n" + "=" * 100)
        print("ç®—æ³•å¯¹æ¯”ï¼ˆå®Œæ•´æŒ‡æ ‡ï¼‰")
        print("=" * 100)
        print(f"{'ç®—æ³•':<15} | {'MAE':>8} | {'RMSE':>8} | {'MAPE':>8} | {'RÂ²':>8} | {'å›æŠ¥':>10} | {'Episodes':>10}")
        print("-" * 100)

        for algo, results in self.results.items():
            final_metrics = results.get('final_metrics', {})
            print(f"{algo.upper():<15} | "
                  f"{final_metrics.get('MAE', 0):>8.4f} | "
                  f"{final_metrics.get('RMSE', 0):>8.4f} | "
                  f"{final_metrics.get('MAPE', 0):>8.2f} | "
                  f"{final_metrics.get('R2', 0):>8.4f} | "
                  f"{final_metrics.get('mean_eval_reward', 0):>10.2f} | "
                  f"{len(results['episode_rewards']):>10}")

    def save_all_results(self):
        """ä¿å­˜æ‰€æœ‰è®­ç»ƒç»“æœ"""
        filepath = os.path.join(CONFIG.vis.RESULTS_DIR, 'training_results.pkl')
        with open(filepath, 'wb') as f:
            pickle.dump(self.results, f)
        print(f"\nâœ“ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {filepath}")

    def load_all_results(self, filepath: str = None):
        """åŠ è½½è®­ç»ƒç»“æœ"""
        if filepath is None:
            filepath = os.path.join(CONFIG.vis.RESULTS_DIR, 'training_results.pkl')

        with open(filepath, 'rb') as f:
            self.results = pickle.load(f)
        print(f"âœ“ è®­ç»ƒç»“æœå·²åŠ è½½: {filepath}")


if __name__ == "__main__":
    print("=" * 60)
    print("è®­ç»ƒæ¨¡å—æµ‹è¯•ï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰")
    print("=" * 60)

    print("\nâœ“ å…³é”®æ”¹è¿›:")
    print("  1. âœ… åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å®Œæ•´æŒ‡æ ‡")
    print("  2. âœ… æ˜¾ç¤ºMAEã€RMSEã€MAPEã€RÂ²ç­‰æ‰€æœ‰æŒ‡æ ‡")
    print("  3. âœ… åŸºäºç»¼åˆæŒ‡æ ‡åˆ¤æ–­æœ€ä½³æ¨¡å‹")
    print("  4. âœ… ä¿å­˜è¯¦ç»†çš„æŒ‡æ ‡å†å²")
    print("  5. âœ… åˆ é™¤æ—©åœæœºåˆ¶ï¼Œå®Œæ•´è®­ç»ƒæ‰€æœ‰episodes")

    print("\n" + "=" * 60)
    print("âœ“ è®­ç»ƒæ¨¡å—å‡†å¤‡å°±ç»ªï¼ˆå®Œæ•´æŒ‡æ ‡ç‰ˆ - æ— æ—©åœï¼‰")
    print("=" * 60)