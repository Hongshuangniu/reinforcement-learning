"""
è¯„ä¼°æ¨¡å— - å®Œå…¨ä½¿ç”¨CONFIGå‚æ•°ï¼ˆé™æ¸©èƒ½åŠ›è¯„ä»·ï¼‰

æ ¸å¿ƒæ”¹è¿›ï¼š
1. âœ… æ‰€æœ‰å‚æ•°ä»CONFIGè¯»å–
2. âœ… å®Œå…¨åŸºäºé™æ¸©èƒ½åŠ›è¯„ä»·
3. âœ… ç§»é™¤å›ºå®šæ¸©åº¦ä¾èµ–
"""

import numpy as np
import pandas as pd
from typing import Dict, List
import os
import pickle

from environment import ImprovedTransformerCoolingEnv
from config import CONFIG
from metrics import MetricsCalculator


class Evaluator:
    """è¯„ä¼°å™¨ï¼ˆå®Œå…¨ä½¿ç”¨CONFIGï¼‰"""

    def __init__(self, env: ImprovedTransformerCoolingEnv, agent, algorithm_name: str):
        self.env = env
        self.agent = agent
        self.algorithm_name = algorithm_name

        # â­ ä½¿ç”¨metrics.pyä¸­çš„è®¡ç®—å™¨ï¼ˆä¸éœ€è¦target_tempï¼‰
        self.metrics_calc = MetricsCalculator()

    def evaluate_episode(self, deterministic: bool = True) -> Dict:
        """
        è¯„ä¼°ä¸€ä¸ªepisodeï¼ˆä½¿ç”¨å®Œæ•´çš„é™æ¸©èƒ½åŠ›è¯„ä»·ä½“ç³»ï¼‰

        Args:
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥

        Returns:
            åŒ…å«å®Œæ•´æŒ‡æ ‡çš„å­—å…¸
        """
        state = self.env.reset()

        temperatures = []  # å®é™…æ¸©åº¦åºåˆ—
        rewards = []
        actions = []
        # ğŸ”¥ é™æ¸©æ•°æ®
        actual_coolings = []
        target_coolings = []

        done = False
        step = 0

        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm_name == 'ppo':
                action, _, _ = self.agent.select_action(state, evaluate=deterministic)
            else:
                action = self.agent.select_action(state, evaluate=deterministic)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)

            # æ”¶é›†æ•°æ®
            temperatures.append(info['oil_temp'])
            rewards.append(reward)
            actions.append(action.copy())
            # ğŸ”¥ æ”¶é›†é™æ¸©æ•°æ®
            actual_coolings.append(info.get('actual_cooling', 0))
            target_coolings.append(info.get('target_cooling', 0))

            state = next_state
            step += 1

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        temperatures = np.array(temperatures)
        actions = np.array(actions)
        actual_coolings = np.array(actual_coolings)
        target_coolings = np.array(target_coolings)

        # â­â­â­ æ ¸å¿ƒï¼šä½¿ç”¨metrics.pyè®¡ç®—æ‰€æœ‰æŒ‡æ ‡ â­â­â­
        all_metrics = self.metrics_calc.calculate_all_metrics(
            temperatures=temperatures,
            rewards=rewards,
            actions=actions,
            actual_coolings=actual_coolings,  # ğŸ”¥ ä¼ å…¥é™æ¸©æ•°æ®
            target_coolings=target_coolings  # ğŸ”¥ ä¼ å…¥é™æ¸©æ•°æ®
        )

        return {
            'temperatures': temperatures,
            'rewards': rewards,
            'actions': actions,
            'actual_coolings': actual_coolings,
            'target_coolings': target_coolings,
            'metrics': all_metrics,
            'total_reward': sum(rewards),
            'avg_temp': np.mean(temperatures),
            'max_temp': np.max(temperatures),
            'min_temp': np.min(temperatures),
            'steps': step,
        }

    def evaluate_multiple_episodes(self, num_episodes: int = None, verbose: bool = True) -> Dict:
        """
        è¯„ä¼°å¤šä¸ªepisodesï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰

        Args:
            num_episodes: è¯„ä¼°episodeæ•°é‡ï¼ˆNoneåˆ™ä»CONFIGè¯»å–ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦

        Returns:
            æ±‡æ€»ç»“æœ
        """
        # ğŸ”¥ ä»CONFIGè¯»å–
        if num_episodes is None:
            num_episodes = CONFIG.train.EVAL_EPISODES

        all_episodes = []
        all_metrics = []

        for i in range(num_episodes):
            episode_data = self.evaluate_episode()
            all_episodes.append(episode_data)
            all_metrics.append(episode_data['metrics'])

            if verbose and (i + 1) % 5 == 0:
                print(f"  è¯„ä¼°è¿›åº¦: {i + 1}/{num_episodes} episodes")

        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
        metrics_summary = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics if key in m]
            if values:
                metrics_summary[key] = np.mean(values)
                metrics_summary[f'{key}_std'] = np.std(values)

        # æ±‡æ€»ç»Ÿè®¡
        summary = {
            'avg_reward': np.mean([ep['total_reward'] for ep in all_episodes]),
            'std_reward': np.std([ep['total_reward'] for ep in all_episodes]),
            'avg_temp': np.mean([ep['avg_temp'] for ep in all_episodes]),
            'max_temp': np.max([ep['max_temp'] for ep in all_episodes]),
            'min_temp': np.min([ep['min_temp'] for ep in all_episodes]),
            'episodes': all_episodes,
            'metrics': metrics_summary,
            'num_eval_episodes': num_episodes  # ğŸ”¥ è®°å½•è¯„ä¼°episodeæ•°
        }

        return summary


class MultiAlgorithmEvaluator:
    """å¤šç®—æ³•è¯„ä¼°å™¨ï¼ˆå®Œå…¨ä½¿ç”¨CONFIGï¼‰"""

    def __init__(self):
        self.results = {}
        self.metrics_calc = MetricsCalculator()

    def evaluate_algorithm(
            self,
            env: ImprovedTransformerCoolingEnv,
            agent,
            algorithm_name: str,
            num_episodes: int = None
    ) -> Dict:
        """
        è¯„ä¼°å•ä¸ªç®—æ³•ï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰

        Args:
            env: ç¯å¢ƒ
            agent: æ™ºèƒ½ä½“
            algorithm_name: ç®—æ³•åç§°
            num_episodes: è¯„ä¼°episodeæ•°é‡ï¼ˆNoneåˆ™ä»CONFIGè¯»å–ï¼‰

        Returns:
            è¯„ä¼°ç»“æœ
        """
        # ğŸ”¥ ä»CONFIGè¯»å–
        if num_episodes is None:
            num_episodes = CONFIG.train.EVAL_EPISODES

        print(f"\nğŸ” è¯„ä¼°ç®—æ³•: {algorithm_name.upper()}")
        print(f"  è¯„ä¼°Episodes: {num_episodes} (æ¥è‡ªCONFIG)")

        evaluator = Evaluator(env, agent, algorithm_name)

        # è¯„ä¼°å¤šä¸ªepisodes
        summary = evaluator.evaluate_multiple_episodes(num_episodes)

        # ä¿å­˜ç»“æœ
        result = {
            'algorithm': algorithm_name,
            'summary': summary,
            'metrics': summary['metrics'],
            'all_episodes': summary['episodes'],
            'config_info': {  # ğŸ”¥ ä¿å­˜CONFIGä¿¡æ¯
                'eval_episodes': num_episodes,
                'best_criterion': CONFIG.metrics.BEST_MODEL_CRITERION,
            }
        }

        self.results[algorithm_name] = result

        # æ‰“å°å…³é”®æŒ‡æ ‡ï¼ˆé™æ¸©èƒ½åŠ›ä¼˜å…ˆï¼‰
        m = summary['metrics']
        print(f"\n  ğŸ”¥ğŸ”¥ğŸ”¥ é™æ¸©èƒ½åŠ›æŒ‡æ ‡ï¼ˆæ ¸å¿ƒï¼‰:")
        print(f"    {CONFIG.metrics.BEST_MODEL_CRITERION}:  {m.get('cooling_mae', 0):8.4f}Â°C  ğŸ‘ˆ ä¸»è¦è¯„ä»·")

        # æ˜¾ç¤ºæ‰€æœ‰é…ç½®çš„ç²¾åº¦é˜ˆå€¼
        for threshold in CONFIG.env.COOLING_PRECISION_THRESHOLDS:
            key = f'cooling_precision_{int(threshold)}c'
            print(f"    ç²¾åº¦Â±{int(threshold)}Â°C:        {m.get(key, 0):8.2f}%")

        print(f"    æ€»é™æ¸©é‡:           {m.get('total_cooling', 0):8.2f}Â°C")
        print(f"    é™æ¸©æ•ˆç‡:           {m.get('cooling_efficiency', 0):8.4f}")

        print(f"\n  ğŸ“Š æ¸©åº¦ç›¸å…³æŒ‡æ ‡ï¼ˆå‚è€ƒï¼‰:")
        print(f"    æ¸©åº¦æ³¢åŠ¨èŒƒå›´:       {m.get('temperature_range', 0):8.2f}Â°C")
        print(f"    æ¸©åº¦æ ‡å‡†å·®:         {m.get('temperature_std', 0):8.4f}Â°C")

        print(f"\n  ğŸ’° å¼ºåŒ–å­¦ä¹ æŒ‡æ ‡:")
        print(f"    å¹³å‡å›æŠ¥:           {m.get('avg_reward', 0):8.2f}")
        print(f"    å›æŠ¥æ ‡å‡†å·®:         {m.get('reward_std', 0):8.4f}")

        return result

    def compare_algorithms(self, save_table: bool = True) -> pd.DataFrame:
        """
        å¯¹æ¯”æ‰€æœ‰ç®—æ³•ï¼ˆä½¿ç”¨CONFIGå‚æ•°ï¼‰

        Args:
            save_table: æ˜¯å¦ä¿å­˜è¡¨æ ¼

        Returns:
            å¯¹æ¯”è¡¨æ ¼
        """
        if not self.results:
            raise ValueError("No evaluation results available.")

        comparison_data = []

        # ğŸ”¥ ä½¿ç”¨CONFIGä¸­å®šä¹‰çš„æŒ‡æ ‡
        for algo_name, result in self.results.items():
            metrics = result['metrics']

            row = {
                'Algorithm': algo_name.upper().replace('_', ' '),
                # ğŸ”¥ é™æ¸©èƒ½åŠ›æŒ‡æ ‡ï¼ˆæ ¸å¿ƒï¼‰
                f'{CONFIG.metrics.BEST_MODEL_CRITERION} (Â°C)': metrics.get('cooling_mae', 0),
            }

            # æ·»åŠ æ‰€æœ‰é…ç½®çš„ç²¾åº¦é˜ˆå€¼
            for threshold in CONFIG.env.COOLING_PRECISION_THRESHOLDS:
                key = f'cooling_precision_{int(threshold)}c'
                row[f'Precision Â±{int(threshold)}Â°C (%)'] = metrics.get(key, 0)

            # å…¶ä»–é™æ¸©æŒ‡æ ‡
            row.update({
                'Total Cooling (Â°C)': metrics.get('total_cooling', 0),
                'Cooling Efficiency': metrics.get('cooling_efficiency', 0),
                'Cooling Stability': metrics.get('cooling_stability', 0),
                # æ¸©åº¦æŒ‡æ ‡ï¼ˆå‚è€ƒï¼‰
                'Temp Range (Â°C)': metrics.get('temperature_range', 0),
                'Temp Std (Â°C)': metrics.get('temperature_std', 0),
                # RLæŒ‡æ ‡
                'Avg Reward': metrics.get('avg_reward', 0),
                'Reward Std': metrics.get('reward_std', 0),
            })

            comparison_data.append(row)

        df = pd.DataFrame(comparison_data)

        if save_table:
            self.save_comparison_table(df)

        return df

    def save_comparison_table(self, df: pd.DataFrame, filename: str = 'algorithm_comparison_cooling_based.csv'):
        """ä¿å­˜å®Œæ•´å¯¹æ¯”è¡¨æ ¼"""
        os.makedirs(CONFIG.vis.TABLE_DIR, exist_ok=True)
        filepath = os.path.join(CONFIG.vis.TABLE_DIR, filename)
        df.to_csv(filepath, index=False, float_format='%.4f')
        print(f"\nâœ“ å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜åˆ°: {filepath}")

    def print_detailed_results(self):
        """æ‰“å°è¯¦ç»†ç»“æœï¼ˆä½¿ç”¨CONFIGæ ¼å¼ï¼‰"""
        print("\n" + "=" * 100)
        print("è¯¦ç»†è¯„ä¼°ç»“æœï¼ˆé™æ¸©èƒ½åŠ›è¯„ä»·ä½“ç³» - æ¥è‡ªCONFIGï¼‰".center(100))
        print("=" * 100)
        print(f"è¯„ä¼°æ ‡å‡†: {CONFIG.metrics.BEST_MODEL_CRITERION} (ä¸»è¦)")
        print(f"           {CONFIG.metrics.SECONDARY_CRITERION} (æ¬¡è¦)")
        print(f"           {CONFIG.metrics.TERTIARY_CRITERION} (ç¬¬ä¸‰)")
        print("=" * 100)

        for algo_name, result in self.results.items():
            print(f"\nç®—æ³•: {algo_name.upper()}")
            print("-" * 100)

            metrics = result['metrics']
            config_info = result.get('config_info', {})

            print(f"é…ç½®: è¯„ä¼°{config_info.get('eval_episodes', 'N/A')}ä¸ªepisodes")

            # ä½¿ç”¨metrics.pyçš„æ‰“å°æ ¼å¼
            self.metrics_calc.print_metrics_summary(metrics)

    def save_all_results(self, filename: str = 'evaluation_results_cooling_based.pkl'):
        """ä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœ"""
        os.makedirs(CONFIG.vis.RESULTS_DIR, exist_ok=True)
        filepath = os.path.join(CONFIG.vis.RESULTS_DIR, filename)

        # åŒ…å«CONFIGä¿¡æ¯
        save_data = {
            'results': self.results,
            'config_snapshot': {
                'eval_episodes': CONFIG.train.EVAL_EPISODES,
                'best_criterion': CONFIG.metrics.BEST_MODEL_CRITERION,
                'secondary_criterion': CONFIG.metrics.SECONDARY_CRITERION,
                'tertiary_criterion': CONFIG.metrics.TERTIARY_CRITERION,
                'cooling_precision_thresholds': CONFIG.env.COOLING_PRECISION_THRESHOLDS,
            }
        }

        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        print("  ï¼ˆåŒ…å«å®Œæ•´CONFIGå¿«ç…§ï¼‰")

    def load_all_results(self, filename: str = 'evaluation_results_cooling_based.pkl'):
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        filepath = os.path.join(CONFIG.vis.RESULTS_DIR, filename)
        with open(filepath, 'rb') as f:
            save_data = pickle.load(f)

        self.results = save_data.get('results', save_data)  # å…¼å®¹æ—§æ ¼å¼

        if 'config_snapshot' in save_data:
            print(f"âœ“ è¯„ä¼°ç»“æœå·²åŠ è½½: {filepath}")
            print("  CONFIGå¿«ç…§:")
            for key, value in save_data['config_snapshot'].items():
                print(f"    {key}: {value}")
        else:
            print(f"âœ“ è¯„ä¼°ç»“æœå·²åŠ è½½: {filepath} (æ—§æ ¼å¼ï¼Œæ— CONFIGå¿«ç…§)")


def generate_evaluation_csv_files(results: Dict, save_dir: str = None):
    """ç”Ÿæˆè¯„ä¼°ç›¸å…³çš„CSVæ–‡ä»¶ï¼ˆä½¿ç”¨CONFIGï¼‰"""
    if save_dir is None:
        save_dir = CONFIG.vis.RESULTS_DIR

    os.makedirs(save_dir, exist_ok=True)

    for algo_name, algo_results in results.items():
        episodes = algo_results['all_episodes']

        # åªä¿å­˜å‰3ä¸ªepisodeçš„è¯¦ç»†æ•°æ®
        for ep_idx, episode in enumerate(episodes[:3]):
            # æ¸©åº¦å’Œé™æ¸©æ•°æ®CSV
            data_df = pd.DataFrame({
                'step': range(len(episode['temperatures'])),
                'temperature': episode['temperatures'],
                'actual_cooling': episode['actual_coolings'],
                'target_cooling': episode['target_coolings'],
            })
            data_df.to_csv(
                os.path.join(save_dir, f'{algo_name}_temp_cooling_ep{ep_idx}.csv'),
                index=False
            )

            # æ§åˆ¶åŠ¨ä½œCSV
            action_df = pd.DataFrame(
                episode['actions'],
                columns=['pressure', 'peltier', 'valve_opening']
            )
            action_df['step'] = range(len(action_df))
            action_df.to_csv(
                os.path.join(save_dir, f'{algo_name}_control_action_ep{ep_idx}.csv'),
                index=False
            )

    print(f"âœ“ è¯„ä¼°CSVæ–‡ä»¶å·²ç”Ÿæˆåˆ°: {save_dir}")


if __name__ == "__main__":
    print("=" * 90)
    print("è¯„ä¼°æ¨¡å—æµ‹è¯•ï¼ˆå®Œå…¨ä½¿ç”¨CONFIGå‚æ•°ï¼‰".center(90))
    print("=" * 90)

    print("\nâœ… æ ¸å¿ƒæ”¹è¿›:")
    print("  1. âœ… æ‰€æœ‰å‚æ•°ä»CONFIGè¯»å–")
    print("  2. âœ… EVAL_EPISODES: CONFIG.train.EVAL_EPISODES")
    print("  3. âœ… æœ€ä½³æ¨¡å‹åˆ¤å®š: CONFIG.metrics.BEST_MODEL_CRITERION")
    print("  4. âœ… é™æ¸©ç²¾åº¦é˜ˆå€¼: CONFIG.env.COOLING_PRECISION_THRESHOLDS")
    print("  5. âœ… å®Œå…¨ç§»é™¤å›ºå®šæ¸©åº¦ä¾èµ–")
    print("  6. âœ… ä½¿ç”¨MetricsCalculatorï¼ˆä¸éœ€è¦target_tempï¼‰")

    print("\nğŸ“Š CONFIGå‚æ•°å±•ç¤º:")
    print(f"  EVAL_EPISODES = {CONFIG.train.EVAL_EPISODES}")
    print(f"  BEST_MODEL_CRITERION = '{CONFIG.metrics.BEST_MODEL_CRITERION}'")
    print(f"  SECONDARY_CRITERION = '{CONFIG.metrics.SECONDARY_CRITERION}'")
    print(f"  TERTIARY_CRITERION = '{CONFIG.metrics.TERTIARY_CRITERION}'")
    print(f"  COOLING_PRECISION_THRESHOLDS = {CONFIG.env.COOLING_PRECISION_THRESHOLDS}")

    print("\n" + "=" * 90)
    print("âœ“ è¯„ä¼°æ¨¡å—ä¿®å¤å®Œæˆï¼ˆå®Œå…¨ä½¿ç”¨CONFIGï¼‰".center(90))
    print("=" * 90)